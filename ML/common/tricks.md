

[神经网络训练技巧汇总(Tricks)](https://mp.weixin.qq.com/s?__biz=MzI4MDE1NjExMQ==&mid=2247504489&idx=1&sn=d4383a7d13b5195f101e4518796e8af8)

[深度学习调优指南](https://github.com/google-research/tuning_playbook)




`模型量化`

模型量化是一种将模型从浮点数形式转换为固定点数形式的过程。

在深度学习中，模型通常以浮点数形式表示权重和偏差。然而，在实际部署和应用中，使用固定点数表示模型可以带来一些优点，比如减少模型的存储空间、加速模型的计算速度和降低模型的功耗等。



`Gradient Checkpoint`

Gradient Checkpoint是一种用于减少内存消耗的方法，常用于训练大型深度学习模型。在深度学习中，反向传播算法需要保存中间计算结果的梯度，以便计算参数的梯度并进行参数更新。然而，对于具有大量参数和层的模型，保存所有梯度可能会占用大量内存。

Gradient Checkpoint通过在反向传播过程中临时计算梯度，而不是一次性保存所有梯度，来减少内存消耗。具体而言，它将模型的计算图分成多个块，并在每个块之间进行梯度计算和参数更新。在每个块的计算过程中，只保存当前块的梯度，并在计算下一个块之前释放前一个块的梯度。这样，可以在不增加内存消耗的情况下，有效地进行反向传播和参数更新。

Gradient Checkpoint方法的一个缺点是会增加计算时间，因为需要多次计算前向传播和反向传播。然而，对于内存受限的情况下，Gradient Checkpoint是一种有效的方法来平衡内存消耗和计算时间。



`添加噪声`

在神经网络中，常用的噪声包括以下几种：

1. 高斯噪声：高斯噪声是一种服从高斯分布的随机噪声，通常用于对输入数据进行扰动。它可以在训练过程中增加模型的鲁棒性，减少过拟合的风险。

2. 随机失活噪声：随机失活噪声是指在网络的某一层或某一部分随机将一些节点置为0。它可以通过减少网络的容量来防止过拟合，并促使网络学习到更加鲁棒的特征。

3. 随机扰动噪声：随机扰动噪声是指在输入数据中添加一些随机扰动，以增加数据的多样性。它可以提高模型的泛化能力，并减少对特定输入样本的过度依赖。

4. Dropout噪声：Dropout是一种通过在训练过程中随机丢弃一些神经元的方法。它可以提高模型的泛化能力，并减少模型对特定神经元的依赖，从而减轻过拟合问题。

这些噪声的使用目的主要有以下几个方面：

1. 提高鲁棒性：模型在面对噪声扰动时更具有稳定性和鲁棒性，能够更好地应对未知的输入数据。

2. 减少过拟合：噪声的引入可以减少模型对训练数据的过度拟合，提高模型的泛化能力。

3. 数据增强：通过对输入数据进行扰动，可以增加数据的多样性，提高模型的学习能力。

4. 防止梯度消失：在深层神经网络中，通过添加噪声可以帮助梯度更好地传播，避免梯度消失问题。




`消融实验`

Ablation Study是一种常用的神经网络实验方法，用于评估和分析神经网络模型的组成部分对模型性能的影响。该方法通过逐步移除模型的不同组件或特征，然后比较模型在移除前后的性能差异，以此来理解这些组件或特征对模型的贡献。

Ablation Study可以用于不同级别的组件或特征的分析。在最基本的层面上，可以通过移除神经网络中的某个层或节点来进行分析。例如，可以移除一个卷积层，然后比较模型在移除前后的性能差异。这样可以帮助我们理解该层对模型的重要性和贡献。

在更高级别上，Ablation Study可以用于分析特征的重要性。例如，可以通过移除某个输入特征或某个特征提取器来进行分析。通过比较模型在移除前后的性能差异，可以了解该特征对模型的影响。

Ablation Study的结果可以帮助研究人员更好地理解神经网络模型，发现模型中关键的组成部分，并优化模型的设计。然而，需要注意的是，Ablation Study只能提供一种较为粗略的分析，因为它是通过逐步删除组件或特征来进行分析的，可能忽略了组件或特征之间的复杂交互作用。因此，Ablation Study通常需要与其他分析方法和实验证据相结合，才能得出全面准确的结论。


一些常见的其他分析方法和实验包括：

1. Sensitivity Analysis（敏感性分析）：通过改变模型的输入或参数，观察模型输出的变化，以评估不同输入或参数对模型的影响。

2. Feature Importance Analysis（特征重要性分析）：通过评估不同特征对模型性能的贡献，确定哪些特征对模型的预测能力最为重要。

3. Cross-validation（交叉验证）：将数据集划分为多个子集，用其中一部分作为验证集，其余部分作为训练集，多次进行训练和验证，以评估模型的泛化能力。

4. Model Comparison（模型比较）：将不同的模型进行对比，评估它们在相同任务上的性能差异，以确定哪个模型更适合解决特定问题。

5. Hyperparameter Tuning（超参数调优）：通过调整模型的超参数，如学习率、正则化参数等，观察模型性能的变化，以找到最佳的超参数配置。

这些方法和实验可以与Ablation Study结合使用，以提供更全面和准确的分析结果，帮助我们更好地理解和优化神经网络模型。


</br>

## _混合精度_

eg. 16 位表示中的计算梯度比 32 位格式快得多，并且还节省了大量内存。这种策略在内存或计算受限的情况下非常有益。

更多训练加速方法：https://mp.weixin.qq.com/s/urhuDZ3Mi_AIUvd1gq91-g



</br>

## _TensorRT_


通过对神经网络进行层合并、权重精度校减（如FP32到FP16或INT8的转换）、内核自动调谐等优化手段，TensorRT 能加速在英伟达GPU上运行的深度学习推断任务。


[tensorRT 加速 stable diffusion](https://nvidia.custhelp.com/app/answers/detail/a_id/5487/~/tensorrt-extension-for-stable-diffusion-web-ui)

-----------

参考资料：
- chatgpt
