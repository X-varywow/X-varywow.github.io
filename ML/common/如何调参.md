
epoch，即将所有数据训练一遍。

batch size；因为显存和算力限制，无法将所有数据一次性用于训练，所以将数据分为 batch；batch size 太小，会使算法不能收敛。增大 batch size 会减少迭代次数。

[深度学习中的batch的大小对学习效果有何影响？](https://www.zhihu.com/question/32673260)，一个很玄学的参数

iteration, 如 6000 的数据集，600 的batch size，每个 epoch 的 iteration 为 10


batch size 不必是 2 的次方，参考：https://blog.51cto.com/u_15298598/5669453


-------------


`mini-batch` 是一种在模型训练时采用的数据划分方式，每次迭代时使用其中一个小批量计算梯度并更新模型参数

介于批量梯度下降（Batch Gradient Descent）和随机梯度下降（Stochastic Gradient Descent, SGD）之间的折中方案；

SGD 每次随机使用一个样本更新参数，波动大，收敛不稳定。

Batch GD 使用全部训练数据计算梯度，开销大但梯度方向稳定。






------------

方法论

（1）当模型预测不准时，并且没有改进思路。可以查看预测不准的数据是什么样子，然后对数据筛选或寻找原因。

（2）[Kaggle 通用方法](https://zhuanlan.zhihu.com/p/27424282)

错误分析->发现新特征->训练新模型->错误分析

（3）了解一个领域，去看综述很有用；杂七杂八的短文，有时不如去上一门专业的课程或一本专业书。



参考：
- https://blog.csdn.net/weixin_33783273/article/details/112949981