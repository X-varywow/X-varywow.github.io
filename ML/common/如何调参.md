
epoch，即将所有数据训练一遍。

batch size；因为显存和算力限制，无法将所有数据一次性用于训练，所以将数据分为 batch；batch size 太小，会使算法不能收敛。增大 batch size 会减少迭代次数。

[深度学习中的batch的大小对学习效果有何影响？](https://www.zhihu.com/question/32673260)，一个很玄学的参数

iteration, 如 6000 的数据集，600 的batch size，每个 epoch 的 iteration 为 10


batch size 不必是 2 的次方，参考：https://blog.51cto.com/u_15298598/5669453

------------

方法论

（1）当模型预测不准时，并且没有改进思路。可以查看预测不准的数据是什么样子，然后对数据筛选或寻找原因。

（2）[Kaggle 通用方法](https://zhuanlan.zhihu.com/p/27424282)

错误分析->发现新特征->训练新模型->错误分析

（3）了解一个领域，去看综述很有用；杂七杂八的短文，有时不如去上一门专业的课程或一本专业书。



参考：
- https://blog.csdn.net/weixin_33783273/article/details/112949981