
epoch，即将所有数据训练一遍。

batch size；因为显存和算力限制，无法将所有数据一次性用于训练，所以将数据分为 batch；batch size 太小，会使算法不能收敛。增大 batch size 会减少迭代次数。

[深度学习中的batch的大小对学习效果有何影响？](https://www.zhihu.com/question/32673260)，一个很玄学的参数

iteration, 如 6000 的数据集，600 的batch size，每个 epoch 的 iteration 为 10


batch size 不必是 2 的次方，参考：https://blog.51cto.com/u_15298598/5669453

------------

参考：
- https://blog.csdn.net/weixin_33783273/article/details/112949981