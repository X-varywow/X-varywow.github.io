
- 熵（信息量的度量）越大，随机变量的不确定性越大。
- 熵代表了随机分布的混乱程度，这一特性是所有基于熵的机器学习算法的核心思想。

---------------------

**自信息**，表示某一事件发生时所带来的信息量的多少。（事件发生的概率越大，则自信息越小，最小为0）
$$I(p_i) = -log(p_i)$$

--------------------

**信息熵**，通常用来描述整个随记分布所带来的信息量平均值，更具有统计特性，在机器学习中，由于熵的计算是依据样本数据而来，故也叫经验熵。`H(X)`

$$H(x)=-\sum_{i=1}^np(x_i)log(p(x_i))$$

>说明：计算公式中的`log`，通常以2为底计算。
> - 以2为底时，熵的单位为：`bits` ；
> - 以e为底时，熵的单位为：`nats`。

规定：0log0 = 0

-------------------

**联合信息熵**,

$$H(X,Y)=-\sum_{i=1}^n\sum_{j=1}^mp(x_i,y_i)log(p(x_i,y_i))$$

**链式规则**,

$$H(X,Y)=H(X)+H(Y|X)$$

----------------------

**条件熵**，在X给定条件下，Y的条件概率分布的熵对X的数学期望。

$$
\begin{aligned}
H(Y|X)&=\sum_{x \in X}p(x)H(Y|X=x)\\
&=\sum_{x \in X}p(x)[-\sum_{y \in Y}p(y|x)log(p(y|x))]\\
&=-\sum_{x \in X}\sum_{y \in Y}p(x,y)log(p(y|x))
\end{aligned}
$$

-----------------------

**互信息**，定义为一个随机变量由于已知另一个随机变量而减少的不确定性。

$$
\begin{aligned}
I(X,Y)&=H(X)-H(X|Y)=H(Y)-H(Y|X)\\
&=H(X)+H(Y)-H(X,Y)\\
&=H(X,Y)-H(X|Y)-H(Y|X)
\end{aligned}
$$

-----------------------

**交叉熵**，用来衡量估计模型与真实概率分布之间的差异情况。

设随机变量$X\sim p(x)$，$q(x)$为用于近似$p(x)$的概率分布。

$$H(X,q)=-\sum_xp(x)log(q(x))$$


**相对熵**，

两个概率分布$p(x)$和$q(x)$的相对熵(或Kullback-Leibler距离，简称为KL距离)，定义为：

$$
\begin{aligned}
KL(p||q)&=H(p,q)-H(p)\\
&=\sum_{k=1}^Np_klog_2\frac{1}{q_k}-\sum_{k=1}^Np_klog_2\frac{1}{p_k}\\
&=\sum_{k=1}^Np_klog_2\frac{p_k}{q_k}
\end{aligned}$$

-------------------------

**信息增益**，表示在一个条件下，信息不确定性减少的程度。

是决策树ID3算法在进行特征切割时使用的划分准则，其物理意义和互信息完全相同，并且公式也是完全相同。其公式：

$$g(D,A)=H(D)-H(D|A)$$

-----------------------

**小结**

1. 自信息时衡量随机变量中的某个时间发生时所带来的信息量的多少，越是不可能发生的事情发生了，那么自信息就越大；
2. 信息熵是衡量随记变量分布的混乱程度，是随记分布个时间发生的自信息的期望值，随机分布越宽广，则熵越大，越混乱；
3. 信息熵推广到多维领域，则可得到联合信息熵；在某些先验条件下，自然引出了条件熵；
4. 前面的熵都是针对一个随机变量的，而交叉熵、相对熵和互信息可以衡量两个随机变量之间的关系，三者作用几乎相同，只是应用范围和领域不同。交叉熵一般用在神经网络和逻辑回归中作为损失函数，相对熵一般用在生成模型中用于评估生成的分布和真实情况的差距，而互信息，则是作为一种评估两个分布之间相似性的数学工具。其三者的关系是：最大化似然函数，等价于最小化负对数似然，等价于最小化交叉熵，等价于最小化KL散度，互信息相对于相对熵区别就是互信息满足对称性；作为熵的典型机器学习算法-决策树，广泛应用了熵进行特征划分，常用的有信息增益、信息增益率和基尼系数。







-----------------------------


_other_

参考资料：
- 学校课程
- https://zhuanlan.zhihu.com/p/35423404
- [信息为什么还有单位，熵为什么用 log 来计算？](https://www.zhihu.com/question/310100965/answer/580684961)

这些，在计算语言学中、机器学习中，都适用

---------------------------

_other_

词内互信息:

$w=c_1,c_2$（$c_1$, $c_2$ 为组成w的汉字），$f(w)$为词频，$f(c_1)$、$f(c_2)$分别为 $c_1$, $c_2$ 在语料库 $C$ 中的出现次数（字频）。且 $C$ 的总词数为 $N_w$，总字数为 $N_c$，则 $w$ 的词内互信息为：

$$mi(w)=log_2(\frac{N^2_c×f(w)}{N_w×f(c_1)×f(c_2)})$$