
<img src="https://img-1301102143.cos.ap-beijing.myqcloud.com/20230726213625.png">

## 1. Python

Numpy 具有 **广播** 功能，不同形状数组也可运算

```python
import numpy as np

A = np.array([1,2],[3,4])
B = np.array([10, 20])

A*B    # -> [[10, 40],[30, 80]]
```

访问元素

```python
X = np.array([[51, 55], [14, 19], [0, 4]])

X[0]

X[0][1]

X = X.flatten() # [51, 55, 14, 19, 0, 4]

X[np.array[0, 2]] # [51, 14]

X > 15 # array([True, True, False ... ], dtype = bool)

X[X>15] # array([51, 55, 19])

np.all(X == 51) # False
```

------------

matplotlib 还具有读取&显示图像的功能

书中例子：
```python
import matplotlib.pyplot as plt
from matplotlib.image import imread

img = imread("demo.png")
plt.imshow(img)

plt.show()
```

在图上绘制散点（人脸关键点检测会用到）(直接 scatter 就行)：

```python
import matplotlib.pyplot as plt

x, y = [i[0] for i in points], [i[1] for i in points]

im = plt.imread("person_head.png")
plt.imshow(im)
plt.scatter(x, y,  c='b', s=10)

plt.show()
```



## 2. 感知机

感知机，就是多个输入、设定权重和阈值，用来判断“神经元是否激活”


```python
def AND(x1, x2):
    w1, w2, theta = 0.5, 0.5, 0.7
    tmp = x1*w1 + x2*w2
    if tmp <= theta:
        return 0
    elif tmp > theta:
        return 1
```


```python
def AND(x1, x2):
    x = np.array([x1, x2])
    w = np.array([0.5, 0.5])
    b = -0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```

单层感知机无法表示异或门，利用多层感知机实现。

感知机通过叠加层能够进行非线性表示。

```python
def XOR(x1, x2):
    s1 = NAND(x1, x2)
    s2 = OR(x1, x2)
    y = AND(s1, s2)
    return y
```
**感知机是神经网络的基础。**



## 3. 神经网络

从前面的感知机来看，**多层感知机可表示万物**，但是就是大量的参数太麻烦了，于是有了 “自动学习” 的网络：

神经网络，能够 **自动地从数据中学习到合适的权重参数**。


这个“自动学习”的过程，就是后续的 BP(反向传播)

----------


一个典型的二层神经网络（只有2层神经元有权重）：

<img src="https://img-1301102143.cos.ap-beijing.myqcloud.com/20230726212411.png" style="zoom:60%">

- 输入层
- 中间层（隐藏层）
- 输出层



------------


P62,简单定义3层神经网络：

```python
import numpy as np

# 定义 sigmoid 函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# 定义 identity 函数，即线性激活函数
def identity_function(x):
    return x

def init_network():
    network = {}
    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])
    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
    network['b2'] = np.array([0.1, 0.2])
    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
    network['b3'] = np.array([0.1, 0.2])
    return network

def forward(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = identity_function(a3)
    return y

network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y)   # [ 0.31682708 0.69627909]
```

现在使用这个简单的网络来拟合 y = x1*x2 + x1 + x2; 这个参数自动调整的过程就是后文的 bp 了


```python
# 损失函数MSE定义
def mean_squared_error(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

# 随机数据生成
def generate_data(num_samples):
    X = np.random.rand(num_samples, 2) * 20 - 10  # 生成样本
    y = X[:, 0] * X[:, 1] + X[:, 0] + X[:, 1]  # 计算真实值
    return X, y

# BP过程
def backpropagation(network, x, y, learning_rate=0.01):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']

    # 前向传播
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y_pred = identity_function(a3)

    # 反向传播
    loss = mean_squared_error(y, y_pred)
    d_loss = y_pred - y
    
    # 计算梯度
    d_a3 = d_loss  # 对最后一层的导数
    d_z2 = d_a3 # 因为恒等函数，梯度不变
    d_W3 = np.dot(z2.T, d_z2) / len(x)
    d_b3 = np.mean(d_z2, axis=0)

    d_a2 = np.dot(d_z2, W3.T) * (z2 * (1 - z2))
    d_W2 = np.dot(z1.T, d_a2) / len(x)
    d_b2 = np.mean(d_a2, axis=0)

    d_z1 = np.dot(d_a2, W2.T) * (z1 * (1 - z1))
    d_W1 = np.dot(x.T, d_z1) / len(x)
    d_b1 = np.mean(d_z1, axis=0)

    # 更新参数
    network['W1'] -= learning_rate * d_W1
    network['b1'] -= learning_rate * d_b1
    network['W2'] -= learning_rate * d_W2
    network['b2'] -= learning_rate * d_b2
    network['W3'] -= learning_rate * d_W3
    network['b3'] -= learning_rate * d_b3

    return loss


network = init_network()
X, y = generate_data(1000)  # 生成1000个样本
y = y.reshape(-1, 1)  # 变形为列向量

epochs = 1000
for epoch in range(epochs):
    loss = backpropagation(network, X, y)
    if epoch % 100 == 0:
        print(f'Epoch {epoch}, Loss: {loss:.4f}')
```


绘制图形：

```python
# 生成数据
x_values = np.linspace(-10, 10, 100)  # 从 -10 到 10 生成 100 个点
X = np.column_stack((x_values, x_values))  # 生成二维输入 (x1, x2)

# 计算真实值
y_true = x_values * x_values + x_values + x_values  # y = x1 * x2 + x1 + x2

y_pred = forward(network, X)

plt.figure(figsize=(10, 6))
plt.plot(x_values, y_true, label='True Function: $y = x_1 * x_2 + x_1 + x_2$', color='blue')
plt.scatter(x_values, y_pred[:,0], label='Network Output', color='red', s=1)
plt.scatter(x_values, y_pred[:,1], label='Network Output', color='green', s=1)
plt.title('Neural Network Output vs True Function')
plt.xlabel('x1 and x2')
plt.ylabel('y')
plt.legend()
plt.grid()
plt.show()
```









------------

激活函数：用于激活的函数。参考 [ML/深度学习/激活函数](ML/深度学习/activatefunc)

回归问题用恒等函数，分类问题用 softmax 函数

P66 softmax 运算中容易越界，改进:

```python
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c) # 溢出对策
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

------------


p74 手写数字识别，输入层784（图像大小28\*28），输出层10\* (10个类别)

都是矩阵，且可以打包式输入数据（批 batch）

?> 数据传送成为瓶颈时，批处理可以减轻数据总线的负荷


```python
x, t = get_data()
network = init_network()
batch_size = 100 # 批数量
accuracy_cnt = 0
for i in range(0, len(x), batch_size):
    x_batch = x[i:i+batch_size]
    y_batch = predict(network, x_batch)
    p = np.argmax(y_batch, axis=1)
    accuracy_cnt += np.sum(p == t[i:i+batch_size])
print("Accuracy:" + str(float(accuracy_cnt) / len(x)))
```

小结：介绍了神经网络基本概念、激活函数、批处理



## 4. 神经网络的学习

“学习” ： 从训练数据中自动获取最优权重参数的过程。

在与门的感知机中，参数只有3个。在具有成千上万个参数的神经网络中，自动设置参数意义重大。

数据驱动，数据是机器学习的核心，从数据中发现模式


------

demo：识别数字 5 的算法

旧方案：从图像中提取特征量（数学方法，SIFT, SURF, ORB等） + 机器学习分类器（SVM, KNN）

缺点：必须人为设计专门的特征量，不够高效

</br>

神经网络方法（学习过程中不存在人为介入），高效

当深度学习可以直接输入原始数据，获得目标结果时，常称为端到端机器学习。


----

#### 4.2 损失函数

神经网络的学习过程（优化过程），需要某个指标为线索寻找最优参数，于是引入了：`损失函数`

一般使用均方误差 MSE、交叉熵误差。


在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为0。P93

（精度是离散的，不能直接用于反向传播，导致参数无法更新）

参考：[评价指标](ML/metric)

----------

mini-batch 学习：从训练数据中选择部分批量数据进行学习

----------

#### 4.3 & 4.4 微分&梯度

```python
# 不好的实现示例
def numerical_diff(f, x):
    h = 10e-50
    return (f(x+h) - f(x)) / h

np.float32(1e-50) # 0.0 计算机无法表示这个数

# 优化1 改变微小值
# 优化2 中心差分替代前向差分
def numerical_diff(f, x):
    h = 1e-4 # 0.0001
    return (f(x+h) - f(x-h)) / (2*h)
```


导数、偏导(某地方变化方向)、梯度（偏导构成的向量）

负梯度方向是梯度法中变量的更新方向。

梯度指示的方向是各点处函数值最小最多的方向。

```python
# 计算梯度
def numerical_gradient(f, x):
    h = 1e-4
    grad = np.zeros_liske(x)

    for i in range(x.size):
        tmp = x[i]

        x[i] = tmp + h
        fxh1 = f(x)
        x[i] = tmp - h
        fxh2 = f(x)

        grad[i] = (fxh1 - fxh2)/(2*h)
        x[i] = tmp
    return grad
```





----------

学习率 $\eta$ （梯度下降中更新程度）

$$
x_0 = x_0 - \eta \frac{\partial f}{\partial x_0} \\
x_1 = x_1 - \eta \frac{\partial f}{\partial x_1} 
$$


算法中会随着时间的推移自主进行修改的值称为 **参数**（parameter），

同时学习算法也会受设定的值所控制（如学习率、batch大小），这些设定的值称为 **超参数**（hyper parameter）

----------

小结：神经网络的学习过程：mini-batch, （反向传播）计算梯度、更新参数，重复训练;

P110 简单代码



## 5. 误差反向传播

误差反向传播，一种高效计算参数权重的梯度的方法。

- 数学式, 计算图
- 链式法则
- 每种运算有其对应的反向传播运算


```python
# 乘法层
class MulLayer:
    def __init__(self):
        self.x = None
        self.y = None
    
    def forward(self, x, y):
        self.x = x
        self.y = y
        return x*y
    
    def backward(self, dout):
        dx = dout * self.y
        dy = dout * self.x
        return dx, dy

```



```python
class Sigmoid:
    def __init__(self):
        self.out = None

    def forward(self, x):
        out = 1/(1 + np.exp(-x))
        self.out = out
        return out

    def backward(self, dout):
        dx = dout * (1.0 - self.out) * self.out
        return dx
```

--------------

神经网络的正向传播中进行的矩阵的乘积运算，在几何中被称为“仿射变换”，这里称其为 “Affine层”


```python
class Affine:
    def __init__(self, w, b):
        self.w = w
        self.b = b
        self.x = None
        self.dw = None
        self.db = None

    def forward(self, x):
        self.x = x
        return np.dot(x, self.w) + self.b

    def backward(self, dout):
        dx = np.dot(dout, self.w.T)
        self.dw = np.dot(self.x.T, dout)
        self.db = np.sum(dout, axis = 0)
        return dx

```

--------

P156 对应误差反向传播法的神经网络的实现



## 6. 与学习相关的技巧

（1）参数的更新（寻找最优参数的过程），SGD(stochastic gradient descent)等，更多参考：[ML/torch 语法/优化器 optimizer](ML/torch/optimizer)

（2）学习率衰减，更多参考：[ML/torch 语法/学习率 lr_scheduler](ML/torch/lr)

（3）**权重参数的初始值** （易被忽视）

不能将权重初始值设成一样的值，这样会在误差反向传播中进行相同的更新。

为了防止“权重均一化”（瓦解权重的对称结构），必须生成随机的初始值。


?>当激活函数为 ReLU 时，权重初始值使用 He 初始值，</br>
当激活函数为 sigmoid 或 tanh 等 S 型曲线函数时，初始值使用 Xavier 初始值。 这是目前的最佳实践。

--------

<u>观察隐藏层的激活值的分布，可以得到很多启发</u>

如：P178 sigmoid隐藏层偏向 01 分布时，导数的值逐渐接近0，是梯度变小。（梯度消失，随着网络层次加深，这个问题会更加严重）

如：P179 激活值十分集中时，会出现“表现力受限”的问题。

--------

（4）数据规整

（希望激活值具有适当的广度，传递多样性的数据，同时进行高效学习）于是有了 BatchNorm 方法（2015年提出，已被广泛使用）


- 可以使学习快速进行(可以增大学习率)。
- 不那么依赖初始值(对于初始值不用那么神经质)。 
- 抑制过拟合(降低Dropout等的必要性)



?> 机器学习有个重要的假设，即训练数据和测试数据满足相同的分布；</br>
</br>随着网络深度加深或在训练过程中，其分布逐渐偏移或变动，①（整体分布逐渐向上下限两端靠近，导致后向传播时低层神经网络的梯度消失，收敛慢。）②（下层输出变动导致上层参数无效）</br>
</br>分布规整操作，可以使得每一层神经网络的输入保持相同分布；</br>
</br>BN 使梯度变大，学习收敛速度快。BN 依赖于batch的大小，当batch值很小时，计算的均值和方差不稳定。</br>
</br>BN, 在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。

---------

（5）过拟合

P191权值衰减、dropout 等正则化方法

（6）超参数优化



## 7. 卷积神经网络

CNN, Convolutional Neural Network

解决全连接层（Affine层）忽视数据形状 的问题（图像被拉平为 1维数据），所以卷积层的输入输出数据，也可称为输入输出特征图

卷积：二维的点积，用于滤波和特征提取

更多卷积相关：https://codec.wang/docs/opencv/basic/extra-08-padding-and-convolution

padding, stride, pooling



## 8. 深度学习

对于手写数字识别任务，不用层次特别深的网络，可以达到 99.38% 精度

人为旋转图像，以扩充数据集 Data Augmentation

P236 手写数字识别CNN特点:
- 基于3×3的小型滤波器的卷积层。 
- 激活函数是ReLU。
- 全连接层的后面使用Dropout层。 
- 基于Adam的最优化。
- 使用He初始值作为权重初始值

----------------

关于加深层的重要性，现状是理论研究还不够透彻，但普遍深层网络具有更强表现力。

深层网络优点：
- 叠加小型滤波器来加深网络可以减少参数的数量，层次越深越明显。
  - 如一次 5x5 卷积运算可由 两次 3x3 卷积运算抵充，参数数量 25 减少为 18。
- 扩大感受野（receptive field, 给神经元施加变化的某个局部空间区域）
- 另外，通过叠加层，将激活函数放在卷积层中间，增加非线性特质，提高网络的表现力

----------------

简单介绍：VGG，GoogLeNet, ResNet

迁移学习、分布式加速、混合精度、多模态等

----------------

`2023.10.07 完`

基础理论还是简单的，多层感知机作为起源，像积木一样把各种层（全连接层、激活层、卷积层等各种运算层）组在一起，构成了一个自动学习机，正向传播可输出结果，反向传播计算梯度用于更新参数。

但各种细节（优化技巧、领域知识）以及新的网络结构，这就很复杂了。










