

实现多卡训练的方式：
- DP (DataParallel)
  - 实现简单但更慢。只能单机多卡使用。GPU 分成 server 节点和 worker 节点，有负载不均衡。
- DDP (DistributedDataParallel)
  - 更快但实现麻烦。可单机多卡也可多机多卡。各个 GPU 是平等的，无负载不均衡。


## DataParallel


## DistributedParallel


- 每个进程对应一个**独立的训练过程**，且只对梯度等少量数据进行信息交换。（梯度汇总再更新）
- 每个进程包含独立的解释器和 GIL


rank, 进程序号, rank=0 为 master 节点

local_rank, 进程内 GPU编号



```bash
python -m torch.distributed.launch --nproc_per_node=8 --master_port=8765 train.py
```


--nproc_per_node：每台机器上要启动的进程数目(number of processes per node)，与 GPU 数目一致


## 框架

accelerate

[30 分钟吃掉 Accelerate 模型训练加速工具](https://huggingface.co/datasets/HuggingFace-CN-community/translation/blob/main/eat_accelerate_in_30_minites.md)



实例：[Training DualStyleGAN 文档](https://github.com/williamyang1991/DualStyleGAN#3-training-dualstylegan)

---------------

参考资料：
- [30 分钟吃掉 Accelerate 模型训练加速工具](https://huggingface.co/datasets/HuggingFace-CN-community/translation/blob/main/eat_accelerate_in_30_minites.md)
- https://docs.oneflow.org/master/parallelism/04_launch.html
- [Pytorch 分布式训练](https://zhuanlan.zhihu.com/p/76638962)
- https://pytorch.org/docs/stable/distributed.html