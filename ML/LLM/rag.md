
RAG, retrieval-augmented generation; æ£€ç´¢å¢å¼ºç”Ÿæˆã€‚

ç›¸å½“äºè¿æ¥ç‰¹å®šçŸ¥è¯†æ•°æ®åº“ï¼Œå¯¹å±€åŸŸçŸ¥è¯†çš„ç´¢å¼•æ£€ç´¢ï¼Œä½¿å…¶èƒ½æ›´å¥½å¤„ç†é—®é¢˜



## æŠ€æœ¯æ ˆ

</br>

å®ç°æ–¹æ³•ï¼š
1. [langchain](https://github.com/langchain-ai/langchain)ï¼Œéœ€è¦å¼€å‘çš„è¾ƒå¤š
2. [ragflow](https://github.com/infiniflow/ragflow)ï¼Œ é€šç”¨ rag æ¡†æ¶
3. [Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat)ï¼Œé€šç”¨ rag æ¡†æ¶ï¼Œ çœ‹ç€å¥½ä¹…æ²¡æ›´æ–°äº†


</br>

å…³é”®æ¨¡å—ï¼š
- æ–‡æœ¬å¤„ç†ï¼Œtext_split
- æ„å»ºå­˜å‚¨å‘é‡ï¼Œembeddingï¼Œvector db
- æ„å»ºç´¢å¼•







## langchain demo


è¯´æ˜ï¼šè£…ç¯å¢ƒ(sagemaker è¿è¡Œçš„)ï¼Œ åé¢å¯ä»¥ gpt ç›´æ¥å¼€å‘ï¼ˆæœ‰äº› gpt è¾“å‡ºçš„åŒ…è¿‡æ—¶äº†ï¼Œ ä»…ç•™æ¡£ï¼‰

å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼šhttps://docs.langchain.com/oss/python/langchain/rag#rag-agents

```bash
# ä½¿ç”¨ base ç¯å¢ƒ python 3.12.11 torch: 2.6.0+cu124

source activate

pip install --upgrade pip
conda update -n base -c conda-forge conda -y
# å¿…é¡»ï¼Œä¸ç„¶åé¢ä¼šæŠ¥é”™


conda install -c conda-forge pyarrow -y
conda install -c conda-forge tiktoken -y

pip install langchain langchain-text-splitters langchain-community langchain_openai
pip install pypdf

pip install sentence-transformers
conda install -c pytorch faiss-gpu -y

python -m ipykernel install --user --name=llm_kernel
```


### 1. data load

```python
docs = []
docs_vis = set()

from langchain_community.document_loaders import PyPDFLoader

pdf_paths = [
    "flow_cn.pdf"
]

def docs_extend(pdf_paths):
    for path in pdf_paths:
        if path in docs_vis:
            print(f"{path} å·²å­˜åœ¨")
            continue
        docs_vis.add(path)
        loader = PyPDFLoader(path)
        docs.extend(loader.load())


docs_extend(pdf_paths)

print(f"âœ… è¿½åŠ  docs {len(docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

```

```python
import re
def clean_pdf_text(text):
    """æ¸…ç† PDF æ–‡æœ¬ä¸­çš„æ¢è¡Œç¬¦å’Œå™ªå£°"""
    
    # 0. é¦–å…ˆè§„èŒƒåŒ–æ‰€æœ‰ç±»å‹çš„ç©ºæ ¼å­—ç¬¦
    # å°†å„ç§ç©ºç™½å­—ç¬¦ï¼ˆåŒ…æ‹¬å…¨è§’ç©ºæ ¼ã€ä¸é—´æ–­ç©ºæ ¼ç­‰ï¼‰ç»Ÿä¸€è½¬æ¢ä¸ºæ™®é€šç©ºæ ¼
    text = re.sub(r'[\u2000-\u200F\u2028-\u202F\u205F\u3000\u00A0\uFEFF]', ' ', text)
    
    # 1. å¤„ç†ä¸­æ–‡æ¢è¡Œé—®é¢˜ - ç§»é™¤ä¸­æ–‡å­—ç¬¦é—´çš„æ¢è¡Œ
    text = re.sub(r'([\u4e00-\u9fff])\n([\u4e00-\u9fff])', r'\1\2', text)
    
    # 2. å¤„ç†è‹±æ–‡æ¢è¡Œé—®é¢˜ - ä¿ç•™å•è¯é—´ç©ºæ ¼
    text = re.sub(r'([a-zA-Z])\n([a-zA-Z])', r'\1 \2', text)
    
    # 3. ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\n+', ' ', text)  # å¤šä¸ªæ¢è¡Œæ›¿æ¢ä¸ºç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)  # å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
    
    # 4. ç§»é™¤å¸¸è§çš„é¡µè„šä¿¡æ¯
    text = re.sub(r'ç¬¬\s*\d+\s*é¡µ', '', text)  # ä¸­æ–‡é¡µç 
    text = re.sub(r'Page\s*\d+', '', text)     # è‹±æ–‡é¡µç 
    text = re.sub(r'\d+\s*$', '', text)        # è¡Œæœ«æ•°å­—
    text = re.sub(r'For More Visit : www.LearnEngineering.in', '', text)
    text = re.sub(r'For More Visit :', '', text)
    
    # 5. æ¸…ç†é¦–å°¾ç©ºç™½
    text = text.strip()
    
    return text


clean_pdf_text(docs[300].page_content)

```

æ¸…ç†æ–‡æœ¬å¹¶æ‹†åˆ†ï¼š

```bash
docs_new = []

for doc in docs:
    doc.page_content = clean_pdf_text(doc.page_content)
    docs_new.append(doc)


from langchain_text_splitters import RecursiveCharacterTextSplitter


# 3ï¸âƒ£ æ‹†åˆ†æ–‡æœ¬ä¸ºå°å—ï¼ˆä¾¿äºå‘é‡åŒ–ï¼‰
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,  # chunk size (characters)
    chunk_overlap=200,  # chunk overlap (characters)
    add_start_index=True,  # track index in original document
)
all_splits = text_splitter.split_documents(docs_new)

print(f"Split docs into {len(all_splits)} sub-documents.")
```




### 2. embedding & vector store

å…ˆ `hf auth login` æ·»åŠ  tokenï¼Œç„¶åå¯ä»¥ä¸‹è½½æ¨¡å‹

```python
from langchain_community.embeddings import HuggingFaceEmbeddings

# ä½¿ç”¨ LangChain çš„ HuggingFace embeddings åŒ…è£…å™¨
embeddings = HuggingFaceEmbeddings(
    model_name="google/embeddinggemma-300m",
    # model_kwargs={'device': 'cpu'},  # æˆ– 'cuda' å¦‚æœæœ‰GPU
    encode_kwargs={'normalize_embeddings': True}  # æ ‡å‡†åŒ–å‘é‡
)

print("âœ… LangChain å…¼å®¹çš„ embedding æ¨¡å‹åŠ è½½æˆåŠŸ")
device = embeddings.client.device
print(f"ğŸ“± HuggingFaceEmbeddings device: {device}")
```

æ³¨æ„ç´¢å¼•åœ¨ from_documents å»ºç«‹äº†ï¼Œ**ä¼šå°†æ‰€æœ‰æ–‡æœ¬å—çš„å‘é‡ï¼Œè¿åŒå®ƒä»¬çš„å…ƒæ•°æ®ï¼ˆå¦‚æ¥æºæ–‡ä»¶ã€é¡µç ã€ç« èŠ‚æ ‡é¢˜ç­‰ï¼‰ï¼Œå­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­**ã€‚ åŒæ—¶å¯é€‰ å…³é”®è¯ç´¢å¼•ï¼ˆä¸ºæ–‡æœ¬å—æ„å»ºä¸€ä¸ªä¼ ç»Ÿçš„å€’æ’ç´¢å¼•ï¼‰

è€Œåé¢ retrieve_context ç§°ä¸ºæ£€ç´¢ï¼Œ ä½¿ç”¨å å‘é‡ç›¸ä¼¼åº¦æœç´¢ï¼Œå…³é”®è¯æœç´¢ç­‰

```python
# way2. ä½¿ç”¨ FAISS å‘é‡å­˜å‚¨

from langchain_community.vectorstores import FAISS

# åˆ›å»º FAISS å‘é‡å­˜å‚¨å¹¶æ·»åŠ æ–‡æ¡£
vector_store = FAISS.from_documents(documents=all_splits, embedding=embeddings)

print(f"âœ… å·²å°† {len(all_splits)} ä¸ªæ–‡æ¡£æ·»åŠ åˆ° FAISS å‘é‡å­˜å‚¨")

# å¯é€‰ï¼šä¿å­˜ FAISS ç´¢å¼•åˆ°æœ¬åœ°æ–‡ä»¶
vector_store.save_local("faiss_index")
print("âœ… FAISS ç´¢å¼•å·²ä¿å­˜åˆ°æœ¬åœ°")

print(f"ğŸ“Š FAISS ç´¢å¼•ç»Ÿè®¡:")
print(f"   - æ–‡æ¡£æ•°é‡: {vector_store.index.ntotal}")
print(f"   - å‘é‡ç»´åº¦: {vector_store.index.d}")
```





### 3. tools & agent

```python
## load local vector_store 

from langchain_community.vectorstores import FAISS

try:
    # å°è¯•åŠ è½½å·²å­˜åœ¨çš„ç´¢å¼•
    vector_store = FAISS.load_local(
        "faiss_index", 
        embeddings, 
        allow_dangerous_deserialization=True
    )
    print("âœ… ä»æœ¬åœ°åŠ è½½ FAISS ç´¢å¼•æˆåŠŸ")
except:
    # å¦‚æœæ²¡æœ‰å·²ä¿å­˜çš„ç´¢å¼•ï¼Œåˆ™é‡æ–°åˆ›å»º
    vector_store = FAISS.from_documents(documents=all_splits, embedding=embeddings)
    vector_store.save_local("faiss_index")
    print("âœ… åˆ›å»ºæ–°çš„ FAISS ç´¢å¼•å¹¶ä¿å­˜åˆ°æœ¬åœ°")

from langchain.tools import tool

@tool(response_format="content_and_artifact")
def retrieve_context(query: str):
    """Retrieve information to help answer a query."""
    retrieved_docs = vector_store.similarity_search(query, k=2)
    serialized = ("\n" + "*" * 50 + "\n").join(
        (f"Source: {doc.metadata}\n\nContent: {doc.page_content}")
        for doc in retrieved_docs
    )
    return serialized, retrieved_docs


```


```python
from langchain.agents import create_agent
from langchain_openai import ChatOpenAI
import os

os.environ["OPENAI_API_KEY"] = "sk-..."
os.environ["OPENAI_API_BASE"] = "https://api.deepseek.com/v1"


# âœ… ä½¿ç”¨ DeepSeek API
llm = ChatOpenAI(
    model="deepseek-chat",  # DeepSeek çš„èŠå¤©æ¨¡å‹å
    # openai_api_key="your_deepseek_api_key_here",
    # openai_api_base="https://api.deepseek.com/v1"
)

# ç®€å•å¯¹è¯
# response = llm.invoke("ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚")
# print(response.content)



tools = [retrieve_context]


# If desired, specify custom instructions
prompt = (
"""
"You have access to a tool that retrieves context from all books. "
"Use the tool to help answer user queries."
"""
)

agent = create_agent(
    llm, 
    tools, 
    system_prompt=prompt
)


query = (
    "ã€‚ã€‚ã€‚\n\n"
    "Once you get the answer, look up common extensions of that method."
)

def ask(query):
    for event in agent.stream(
        {"messages": [{"role": "user", "content": query}]},
        stream_mode="values",
    ):
        event["messages"][-1].pretty_print()


ask(query)
```


--------------

å‚è€ƒèµ„æ–™ï¼š
- [rag ç»¼è¿°](https://zhuanlan.zhihu.com/p/683651359)
- gpt