
`tokenization`

## 1 利用分词器构建词汇表

#### 1.1 独热向量模式

```python
import numpy as np
import pandas as pd
sentence = """Thomas Jefferson began building Monticello at the age of 26."""
token_sequence = sentence.split()
vocab = sorted(set(token_sequence)) # 词汇表

num_tokens = len(token_sequence)
vocab_size = len(vocab)
onehot_vectors = np.zeros((num_tokens, vocab_size), int)

for i, word in enumerate(token_sequence):
    onehot_vectors[i,vocab.index(word)] = 1
    
df = pd.DataFrame(onehot_vectors, columns=vocab)
df[df==0] = ''  # 为了美观，一般不用
df
```

![](https://s2.loli.net/2022/01/04/nZWrM7bIdlCoJyE.jpg)

#### 1.2 词袋向量模式

```python
import pandas as pd
raw_data = """We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. """
corpus = {}

for i, sent in enumerate(raw_data.split(".")):
    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())
    
#print(corpus) # 一个二层字典
df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T
df.head()
```

![](https://s2.loli.net/2022/01/04/6EC89NUIYMXeGah.jpg)

?> `format` 不仅仅可以用在输出中<br> `dict()` 中塞入二维元组可以自动 key, val <br> `from_records(corpus).fillna(0)`

>或者：使用 sklearn 中的 CountVectorizer

```python
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
count = CountVectorizer()

raw_data = """We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25× fewer parameters. """
corpus = raw_data.split(".")

bag = count.fit_transform(corpus)
print(bag)   # 稀疏矩阵 <class 'scipy.sparse.csr.csr_matrix'>
print(count.vocabulary_)
print(bag.toarray())
```

>或者：使用工具包：NLTK， StanfordCoreNLP，jieba

## 2 正向最大匹配分词

```python
cidian = set()
with open("word.txt","r",encoding="utf-8") as f:
    ci = f.readline().strip("\n")
    while ci:
        #print(len(ci),ci[0],ci[-1])
        cidian.add(ci)
        ci = f.readline().strip("\n")

def main(s):
    n = len(s)
    ans = []
    l = 0
    while l < n:
        tmp = s[l]
        for r in range(l,n):
            cur_ci = s[l:r+1]
            print(cur_ci,l,r+1)
            if cur_ci in cidian:
                tmp = cur_ci
        ans.append(tmp)
        l += len(tmp)
    return ans

main("沧浪寄余生")

# set 查找效率近似于O（1），同 dict
# strip（）去除字符首尾的指定字符
# cidian = set([i.strip() for i in open().readlines()])
```

```输出
沧 0 1
沧浪 0 2
沧浪寄 0 3
沧浪寄余 0 4
沧浪寄余生 0 5
寄 2 3
寄余 2 4
寄余生 2 5
余 3 4
余生 3 5
['沧浪', '寄', '余生']
```