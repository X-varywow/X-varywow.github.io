
## preface

做这个主要是为 TTS 能够生成特定情感基调的语音

?> wav2vec 自监督训练相当于**训练了一个特征提取器**，hubert 也是一个类似的自监督表征模型。</br>这相当于 word2vec 是单词的 representation，wav2vec 是音频的 representation。</br>
</br>vec 表征可用于情感分类等任务



## models


[xlsr-wav2vec-speech-emotion-recognition](https://huggingface.co/harshit345/xlsr-wav2vec-speech-emotion-recognition)

[wav2vec2-lg-xlsr-en-speech-emotion-recognition](https://huggingface.co/ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition)


可以跑通的代码：

```python
!git clone https://github.com/m3hrdadfi/soxan.git
%cd soxan
```


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
from transformers import AutoConfig, Wav2Vec2FeatureExtractor
from src.models import Wav2Vec2ForSpeechClassification, HubertForSpeechClassification
import librosa
import IPython.display as ipd
import numpy as np
import pandas as pd
from pprint import pprint


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model_name_or_path = "harshit345/xlsr-wav2vec-speech-emotion-recognition"

config = AutoConfig.from_pretrained(model_name_or_path)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name_or_path)
sampling_rate = feature_extractor.sampling_rate

# for wave2vec
model = Wav2Vec2ForSpeechClassification.from_pretrained(model_name_or_path).to(device)

# for hubert
# model = HubertForSpeechClassification.from_pretrained(model_name_or_path).to(device)


def speech_file_to_array_fn(path, sampling_rate):
    speech_array, _sampling_rate = torchaudio.load(path)
    resampler = torchaudio.transforms.Resample(_sampling_rate)
    speech = resampler(speech_array).squeeze().numpy()
    return speech

def predict(path, sampling_rate):
    speech = speech_file_to_array_fn(path, sampling_rate)
    inputs = feature_extractor(speech, sampling_rate=sampling_rate, return_tensors="pt", padding=True)
    inputs = {key: inputs[key].to(device) for key in inputs}
    with torch.no_grad():
        logits = model(**inputs).logits
    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]
    outputs = [{"Emotion": config.id2label[i], "Score": f"{round(score * 100, 3):.1f}%"} for i, score in enumerate(scores)]
    return outputs

path = "/home/ec2-user/SageMaker/LJSpeech-1.1/wavs/LJ001-0067.wav"
outputs = predict(path, sampling_rate)
pprint(outputs)
print(f"sampling_rate: {sampling_rate}")

# [{'Emotion': 'anger', 'Score': '2.0%'},
#  {'Emotion': 'disgust', 'Score': '19.7%'},
#  {'Emotion': 'fear', 'Score': '0.2%'},
#  {'Emotion': 'happiness', 'Score': '77.4%'},
#  {'Emotion': 'sadness', 'Score': '0.7%'}]
# sampling_rate: 16000
```


--------------

参考资料：
- [Wav2vec——无监督的Speech Representation预训练模型](https://zhuanlan.zhihu.com/p/272013702)



