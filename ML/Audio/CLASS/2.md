## 9. 学习率策略和梯度裁剪策略

```python
# train1.py

import torch
from model import Network
from data import MyData

from torch.utils.data.dataloader import DataLoader

def train(dirname):
    net = Network()
    adam = torch.optim.Adam(net.parameters(), lr=0.001)

    dataset = MyData(dirname)
    dataloader = DataLoader(dataset, batch_size=10, num_workers=5, shuffle=True)

    for epo in range(1000): # 数据进行100次迭代
        for i, (x, y) in enumerate(dataloader): # 在后台预读数据
            y_pred = net(x)
            loss = torch.nn.CrossEntropyLoss()(y_pred, y)

            adam.zero_grad()
            loss.backward()
            adam.step()

            error = (y != y_pred.argmax(-1)).sum()

            lr = adam.param_groups[0]['lr'] 
            
            print('epo %03d step %03d: loss %.4f'%(epo, i, loss.cpu().item()),
                   error, y, y_pred.argmax(-1), lr)

if __name__ == "__main__":
    train('/home/yz2018/asr2022/database')
```

```python
# train2.py

import torch
from model import Network
from data import MyData

from torch.utils.data.dataloader import DataLoader

def train(dirname):
    net = Network()
    adam = torch.optim.Adam(net.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.LambdaLR(adam, lambda step: min([1, (step+1)/3000]) )

    dataset = MyData(dirname)
    dataloader = DataLoader(dataset, batch_size=10, num_workers=5, shuffle=True)

    for epo in range(1000): # 数据进行100次迭代
        for i, (x, y) in enumerate(dataloader): # 在后台预读数据
            y_pred = net(x)
            loss = torch.nn.CrossEntropyLoss()(y_pred, y)

            scheduler.step()  #依据学习率策略step
            adam.zero_grad()
            loss.backward()
            adam.step()

            error = (y != y_pred.argmax(-1)).sum()

            lr = adam.param_groups[0]['lr'] 
            
            print('epo %03d step %03d: loss %.4f'%(epo, i, loss.cpu().item()),
                   error, y, y_pred.argmax(-1), lr)

if __name__ == "__main__":
    train('/home/yz2018/asr2022/database')
```

```python
# train3.py

import torch
from model import Network
from data import MyData

from torch.utils.data.dataloader import DataLoader

def train(dirname):
    net = Network()
    adam = torch.optim.Adam(net.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.LambdaLR(adam, lambda step: min([1, (step+1)/300, 300/(step+1)]) )
    torch.optim.lr_scheduler.CyclicLR
    dataset = MyData(dirname)
    dataloader = DataLoader(dataset, batch_size=10, num_workers=5, shuffle=True)

    for epo in range(1000): # 数据进行100次迭代
        for i, (x, y) in enumerate(dataloader): # 在后台预读数据
            y_pred = net(x)
            loss = torch.nn.CrossEntropyLoss()(y_pred, y)

            scheduler.step()
            adam.zero_grad()
            loss.backward()
            adam.step()

            error = (y != y_pred.argmax(-1)).sum()

            lr = adam.param_groups[0]['lr'] 
            
            print('epo %03d step %03d: loss %.4f'%(epo, i, loss.cpu().item()),
                   error, y, y_pred.argmax(-1), lr)

if __name__ == "__main__":
    train('/home/yz2018/asr2022/database')
```

## 10. 分布不平稳与分布规整



```python
# model.py

import torch
from torch.nn import Linear, Conv2d, Sequential, BatchNorm2d

class Network(torch.nn.Module): # LeNet
    def __init__(self):
        super().__init__()
        self.conv1 = Conv2d(1, 1, (5,5), stride=(1,2), padding=(2,2)) # [B, 1, 24, 100] -> [B, 1, 24, 50]
        self.conv2 = Conv2d(1, 1, (5,5), stride=(1,2), padding=(2,2)) # [B, 1, 24, 50] -> [B, 1, 24, 25]

        self.conv3 = Conv2d(1, 1, (5,5), stride=(2,2), padding=(2,2)) # [B, 1, 24, 25] -> [B, 1, 12, 13]
        self.conv4 = Conv2d(1, 1, (5,5), stride=(2,2), padding=(2,2)) # [B, 1, 12, 13] -> [B, 1, 6, 7]

        self.linear1 = Linear(6*7, 128)
        self.linear2 = Linear(128, 10)

    def forward(self, x):
        x = self.conv1(x).relu()
        x = self.conv2(x).relu()
        x = self.conv3(x).relu()
        x = self.conv4(x).relu() # [B, 1, 6, 7]

        x = x.reshape(-1, 6*7) # [B, 42]

        x = self.linear1(x).sigmoid() # [B,42] -> [B, 64]
        x = self.linear2(x) # .softmax(-1) # [B, 64] -> [B, 10]
        return x

if __name__ == "__main__":

    net = Network()
    x = torch.randn(4, 1, 24, 100)
    y = net(x).softmax(-1)
    print(x.shape, y.shape)
    print(y)

```

```python
# data.py

import os, torch, numpy as np
import torchaudio, torchvision
from torchaudio.transforms import *

class MyData(torch.utils.data.Dataset):
    def __init__(self, dirname):
        super().__init__()
        self.names = sorted(['%s/%s'%(dirname, x) for x in os.listdir(dirname)])
        self.fbank = MelSpectrogram(sample_rate=16000, n_fft=512, win_length=512,
                                     hop_length=160, n_mels=24, normalized=True)
    def __len__(self): 
        return len(self.names)

    def __getitem__(self, index):
        x, fs = torchaudio.load(self.names[index])
        if x.shape[0]>1: x = x[0:1, :]
        if fs != 16000:  x = Resample(fs, 16000)(x)
        x = self.fbank(x).clamp(1e-5).log10() # [1, F, T]
        x = torchvision.transforms.Resize((24,100))(x)
        y = int(self.names[index][-5])
        return x, y

if __name__ == "__main__":

    data = MyData('/home/yz2018/asr2022/database')
    x, y = data.__getitem__(9)
    print(len(data), x.shape, y)
```

```python
# train.py

import torch
from model import Network
from data import MyData

from torch.utils.data.dataloader import DataLoader

def train(dirname):
    net = Network()
    adam = torch.optim.Adam(net.parameters(), lr=0.001)

    dataset = MyData(dirname)
    dataloader = DataLoader(dataset, batch_size=10, num_workers=5, shuffle=True)

    for epo in range(1000): # 数据进行100次迭代
        for i, (x, y) in enumerate(dataloader): # 在后台预读数据
            y_pred = net(x)
            loss = torch.nn.CrossEntropyLoss()(y_pred, y)

            adam.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad.clip_grad_value_(net.parameters(), 1.0)
            #torch.nn.utils.clip_grad.clip_grad_norm_(net.parameters(), 1.0)

            adam.step()

            error = (y != y_pred.argmax(-1)).sum()
            
            print('epo %03d step %03d: loss %.4f'%(epo, i, loss.cpu().item()),
                   error, y, y_pred.argmax(-1), )

if __name__ == "__main__":
    train('/home/yz2018/asr2022/database')
```

## 11. 不定长时间序列

- 循环神经网络：能够记忆历史的网络
  - `RNN`
  - `LSTM`
    - 长短时记忆网络（LSTM，Long Short-Term Memory）是一种时间循环神经网络，是为了解决一般的RNN（循环神经网络）存在的长期依赖问题而专门设计出来的。
  - `GRU`
    - 门控循环网络

## 12. CTC 技术

Connectionist temporal classification，是一种常用在语音识别、文本识别等领域的算法，用来解决输入和输出序列长度不一、无法对齐的问题。

<img src="https://img-1301102143.cos.ap-beijing.myqcloud.com/20230420220633.png">

参考：[CTC理论和实战](http://fancyerii.github.io/books/ctc/)

## 13. 多头注意力模型

注意力：按照相似度筛选源信息

$$Softmax(Q^TK)V$$

Q(目标数据)
K(源数据)
V(源数据)

## 14. transformer

参考 ML/深度学习/transformer

模型改进：
- 残差连接，可构造“极深的”神经网络
- 分布规整
- dropout，随机置零，每次随机丢弃不同节点，使每次迭代都训练不同的子网。提高泛化能力，只在训练时起作用
- 位置无关问题：positional encoding，循环神经，卷积神经

## 15. transformer 掩膜机制

忽略某些位置，不计算与其相关的注意力权重。

参考：[Mask机制](https://ifwind.github.io/2021/08/17/Transformer%E7%9B%B8%E5%85%B3%E2%80%94%E2%80%94%EF%BC%887%EF%BC%89Mask%E6%9C%BA%E5%88%B6/)

## other

优化初始化参数 & 收敛速度

torch.nn.init.xavier_uniform_(self.conv.weight, gain=torch.nn.init.calculate_gain("relu"))

torch.nn.init.kaiming_uniform_(self.conv.weight, nonlinearity="relu")


-------------

可分离卷积技术

缩小模型方法：空间分离卷积、深度分离卷积、

--------------

数据增强技术
