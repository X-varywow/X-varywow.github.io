

?> 机器学习有个重要的假设，即训练数据和测试数据满足相同的分布；</br>
</br>随着网络深度加深或在训练过程中，其分布逐渐偏移或变动，①（整体分布逐渐向上下限两端靠近，导致后向传播时低层神经网络的梯度消失，收敛慢。）②（下层输出变动导致上层参数无效）</br>
</br>分布规整操作，可以使得每一层神经网络的输入保持相同分布；</br>
</br>BN 使梯度变大，学习收敛速度快。BN 依赖于batch的大小，当batch值很小时，计算的均值和方差不稳定。</br>
</br>BN, 在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。


常见分布规整操作：
- BatchNorm
- LayerNorm
- InstanceNorm
- GroupNorm


---------



使用 layernorm:

```python
import torch
from torch import nn

# NLP Example
batch, sentence_length, embedding_dim = 20, 5, 10
embedding = torch.randn(batch, sentence_length, embedding_dim)

# Activate module
layer_norm = nn.LayerNorm(embedding_dim)
print(layer_norm(embedding))

# Image Example
N, C, H, W = 20, 5, 10, 10
input = torch.randn(N, C, H, W)

# Normalize over the last three dimensions (i.e. the channel and spatial dimensions)
# as shown in the image below
layer_norm = nn.LayerNorm([C, H, W])
output = layer_norm(input)
print(output)
```


使用batchnorm1d:

```python
# With Learnable Parameters
m = nn.BatchNorm1d(100)
# Without Learnable Parameters
m = nn.BatchNorm1d(100, affine=False)
input = torch.randn(20, 100)
output = m(input)
```









---------

参考资料：
- [BatchNorm的原理及代码实现](https://zhuanlan.zhihu.com/p/88347589)
- https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html