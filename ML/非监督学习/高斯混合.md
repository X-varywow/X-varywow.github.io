

## _GaussiansMixture_

单变量x的高斯分布，其概率分布如下：

$$N(x;\mu,\sigma) = \frac{1}{\sqrt{2\pi \sigma ^2}}exp[-\frac{(x-\mu)^2}{2\sigma ^2}]$$

现在将数据扩展到 d 维：

$$N(x;\mu,\Sigma) = \frac{1}{\sqrt{(2\pi)^d det(\Sigma)}}exp[-\frac12(x-\mu)\Sigma^{-1}(x-\mu)^T]$$

$\Sigma$ 是一个 d×d 矩阵，代表模型的协方差矩阵


kmeans 硬分类，高斯混合 后者软分类






## _demo_

```python
import torch
from torch.distributions import Normal
from torch.nn.functional import softplus

# 构建混合高斯模型
class MixtureOfGaussians(torch.nn.Module):
    def __init__(self, n_components):
        super().__init__()
        # 初始化参数：均值、方差、混合系数
        self.means = torch.nn.Parameter(torch.randn(n_components))
        self.log_vars = torch.nn.Parameter(torch.randn(n_components))  # 使用log方差为了保持数值稳定性
        self.weights = torch.nn.Parameter(torch.randn(n_components))

    def forward(self, x):
        # 计算各高斯分量的概率密度
        total_prob = torch.zeros_like(x)
        variances = torch.exp(self.log_vars)
        
        # 确保权重是正的并且和为1
        weights = torch.nn.functional.softmax(self.weights, dim=0)

        for mean, var, weight in zip(self.means, variances, weights):
            normal_dist = Normal(mean, var.sqrt())
            total_prob += weight * torch.exp(normal_dist.log_prob(x))
        
        return total_prob

# 生成一个双峰分布的数据样本
def create_bimodal_data(n_data, mean1, std1, mean2, std2):
    data1 = torch.normal(mean1, std1, size=(n_data,))
    data2 = torch.normal(mean2, std2, size=(n_data,))
    return torch.cat([data1, data2])

# 损失函数，这里使用负对数似然损失
def negative_log_likelihood(x, model):
    return -torch.sum(torch.log(model(x) + 1e-6))  # 添加小常数防止对数为负无穷

# 训练模型
def train_model(model, data, optimizer):
    model.train()
    optimizer.zero_grad()  # 清除之前的梯度
    loss = negative_log_likelihood(data, model)
    loss.backward()  # 计算梯度
    optimizer.step()  # 更新参数
    return loss.item()

n_components = 2  # 分量数为2，因为是双峰分布
model = MixtureOfGaussians(n_components)
optimizer = torch.optim.Adam(model.parameters())

# 生成数据
n_data = 500
data = create_bimodal_data(n_data, mean1=-2, std1=0.5, mean2=2, std2=0.5)

# 训练循环
epochs = 1000
for epoch in range(epochs):
    loss = train_model(model, data, optimizer)
    if epoch % 100 == 0:
        print(f'Epoch {epoch}: loss {loss}')
```







----------------


参考资料：
- [如何通俗的理解高斯混合模型](https://zhuanlan.zhihu.com/p/151671154)
- [混合高斯模型-EM算法](https://zhuanlan.zhihu.com/p/326055752)
- chatgpt