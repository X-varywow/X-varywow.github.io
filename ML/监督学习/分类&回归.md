
## _分类_

k 近邻

支持向量机 SVM

决策树

朴素贝叶斯

------------

ROC curve, Receiver Operating Characteristic curve 通常用于评估分类模型

ROC曲线通过将模型在不同阈值下的真正例率（True Positive Rate, TPR）和假正例率（False Positive Rate, FPR）画在图上来展现模型的性能。

绘制ROC曲线的步骤通常如下：

1. 对于二分类模型的输出（一般是某一类的概率），设定多个不同的阈值。对于每一个阈值，将模型预测的结果分为正类和负类。
2. 计算出每个阈值对应的TPR和FPR。
3. 在图上绘制出所有计算出的（FPR, TPR）点并进行连线，形成的曲线即为ROC曲线。

ROC曲线下的面积称为AUC（Area Under the ROC Curve），范围从0到1。AUC值越大，表示模型的性能越好，具有更好的区分正负类的能力。一个完美的分类模型的AUC为1，而一个完全随机的分类模型的AUC值为0.5。

-----------

混淆矩阵

```python
from sklearn.metrics import confusion_matrix

# 进行预测
y_pred = model.predict(X_test, num_iteration=model.best_iteration_)

# 将预测结果转换为类别标签
y_pred_labels = [y_pred[i].argmax() for i in range(len(y_pred))]

# 计算混淆矩阵
cm = confusion_matrix(y_test, y_pred_labels)

# 打印混淆矩阵
print("Confusion Matrix:")
print(cm)
```

classification_report：

```python
from sklearn.metrics import classification_report

# 计算每个类别的正确率
report = classification_report(y_test, y_pred_labels)
print(report)

```




</br>

## _回归_

回归问题是指预测一个连续数值的问题，不同于分类问题输出一个离散的类别标签。

应用领域：房价预测、股票价格预测、销量预测等。


在回归问题中，通常会使用一些特征来描述输入数据，并且需要有一些标记好的数据集来训练模型。

常见的回归算法包括
- 线性回归
- 多项式回归
- 百分位回归
- 支持向量回归
- 决策树回归
- 随机森林回归
- 神经网络回归等

----------

LWLR， 局部加权线性回归（Locally Weighted Linear Regression），是一种改善传统线性回归模型适应性的机器学习算法。

很好地适用于数据非线性关系较强的情况，尤其是在存在高局部变化的数据集上；但是计算成本高，依赖于先验的权重函数选择。

----------

对于回归问题，常见的处理方法包括特征工程、模型选择和调参等。特征工程可以通过选择合适的特征、进行特征变换和特征组合等方式来提取更有用的特征。模型选择可以根据实际情况选择适合的回归算法。调参可以通过交叉验证等方法来选择最优的模型参数。







----------------

参考资料：
- [10 Most Common Machine Learning Algorithms Explained -2023](https://medium.com/@riteshgupta.ai/10-most-common-machine-learning-algorithms-explained-2023-d7cfe41c2616)
- 地毯书，13章：分类
- chatgpt