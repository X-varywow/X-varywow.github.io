


[google developer æ–‡æ¡£](https://developers.google.com/mediapipe/solutions/guide) 

githubåœ°å€ï¼šhttps://github.com/google/mediapipe

ã€ä»‹ç»ã€‘è§†é¢‘å’Œå›¾ç‰‡çš„æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆ

ã€åŠŸèƒ½ã€‘è„¸éƒ¨æ£€æµ‹ã€æ‰‹åŠ¿æ£€æµ‹ã€å§¿æ€æ£€æµ‹ç­‰ã€‚

ã€æ”¯æŒã€‘web, python, Android

```bash
pip install mediapipe
```



## Face Detection

å‚è€ƒï¼š[colab ä»£ç ](https://colab.research.google.com/drive/1JaQL7MnsH1NN6i5wsloWlIYhoUC4QZOh)


```python
import mediapipe as mp
import matplotlib.pyplot as plt
import math


mp_face_detection = mp.solutions.face_detection

# help(mp_face_detection.FaceDetection)

short_range_images = {name: cv2.imread(name) for name in ['person_head.png']}

DESIRED_HEIGHT = 480
DESIRED_WIDTH = 480
def resize_and_show(image):
    h, w = image.shape[:2]
    if h < w:
        img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
    else:
        img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
    # cv2_imshow(img)
    # cv2_imshow æœ‰å¯èƒ½ä¼šæŠ¥é”™
    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))  # è½¬æ¢æ ¼å¼ä»¥æ­£å¸¸æ˜¾ç¤ºï¼Œä¸ç„¶è“è“çš„

mp_drawing = mp.solutions.drawing_utils
drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)

# é€šè¿‡ model_selection=1 å¯ä»¥æŒ‡å®š full range model
# model_selection=0 é€‚ç”¨äº å¤§å¤´ç…§
with mp_face_detection.FaceDetection(
    min_detection_confidence=0.5,
    model_selection=0) as face_detection:
    for name, image in short_range_images.items():
        res = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

        print(f"Face detections of {name}:")
        if not res.detections:
            continue
        annotated_image = image.copy()
        for detection in res.detections:
            mp_drawing.draw_detection(annotated_image, detection)
        resize_and_show(annotated_image)
```


## Face landmarks ğŸ”

[å®˜æ–¹DEMOæ¼”ç¤º](https://mediapipe-studio.webapps.google.com/studio/demo/face_landmarker)

> åŠŸèƒ½ï¼šå®æ—¶è®¡ç®— **478ä¸ªé¢éƒ¨ç‚¹ä½** å’Œ **52ä¸ªé¢éƒ¨ç‰¹å¾**ã€‚å¯ç”¨äºè™šæ‹Ÿä¸»æ’­ã€è§†é¢‘åŠ¨æ•ç­‰ã€‚

è®¡ç®—çš„æ˜¯3D ç©ºé—´çš„ç‚¹ä½ä¿¡æ¯ï¼ŒåŒ…å« z è½´æ–¹å‘ä¿¡æ¯

åŒ…å«äº†3ä¸ªæ¨¡å‹ [ä¸‹è½½åœ°å€](https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task)


| æ¨¡å‹         | è¾“å…¥        | ç»“æ„                                                                                              | è¯´æ˜         |
| ------------ | ----------- | ------------------------------------------------------------------------------------------------- | ------------ |
| FaceDetector | 192 x 192   | Convolutional Neural Network: [SSD](https://arxiv.org/abs/1512.02325)-like with a custom encoder. | éƒ½æ˜¯ float16 |
| FaceMesh-V2  | 256 x 256   | Convolutional Neural Network: [MobileNetV2](https://arxiv.org/abs/1801.04381)                     |              |
| Blendshape   | 1 x 146 x 2 | [MLP-Mixer](https://keras.io/examples/vision/mlp_image_classification/#the-mlpmixer-model)        |              |



other: ä½¿ç”¨ Effect Renderer å¯ä»¥æ·»åŠ ç‰¹æ•ˆè´´å›¾ç­‰ç­‰ï¼Œ

### ç¤ºä¾‹1

å‚è€ƒï¼š[colab ä»£ç ](https://colab.research.google.com/drive/1FCxIsJS9i58uAsgsLFqDwFmiPO14Z2Hd)

```python
import mediapipe as mp
mp_face_mesh = mp.solutions.face_mesh
help(mp_face_mesh.FaceMesh)

mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

images = {name: cv2.imread(name) for name in ['person_head.png']}

with mp_face_mesh.FaceMesh(
    static_image_mode = True,
    refine_landmarks = True,
    max_num_faces = 2,
    min_detection_confidence = 0.5) as face_mesh:

    for name, image in images.items():
        res = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        print(f"Face landmarks of {name}:")
        if not res.multi_face_landmarks:
            continue
        annotated_image = image.copy()
        for face_landmarks in res.multi_face_landmarks:

            # ç”»å‡ºç°è‰²çš„è„¸éƒ¨ç½‘æ ¼
            mp_drawing.draw_landmarks(
                image = annotated_image,
                landmark_list = face_landmarks,
                connections = mp_face_mesh.FACEMESH_TESSELATION,
                landmark_drawing_spec = None,
                connection_drawing_spec = mp_drawing_styles.get_default_face_mesh_tesselation_style()
            )
        resize_and_show(annotated_image)
```


æ·»åŠ å…¶ä»–çš„ç»˜åˆ¶ï¼š

```python
# ç»˜åˆ¶è½®å»“ï¼ŒåŒ…æ‹¬ï¼šçœ¼ç›ã€è„¸éƒ¨ã€çœ‰æ¯›ã€å˜´å·´
mp_drawing.draw_landmarks(
    image = annotated_image,
    landmark_list = face_landmarks,
    connections = mp_face_mesh.FACEMESH_CONTOURS,
    landmark_drawing_spec = None,
    connection_drawing_spec = mp.drawing_styles.get_default_face_mesh_contours_style()
)

# ç»˜åˆ¶è™¹è†œï¼ˆå››è¾¹å½¢ï¼‰
mp_drawing.draw_landmarks(
    image=annotated_image,
    landmark_list=face_landmarks,
    connections=mp_face_mesh.FACEMESH_IRISES,
    landmark_drawing_spec=None,
    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_iris_connections_style()
)
```








### ç¤ºä¾‹2

å‚è€ƒ: [github face_mesh](https://github.com/google/mediapipe/blob/master/docs/solutions/face_mesh.md)

åŸºæœ¬æµç¨‹ï¼š

```python
import mediapipe as mp
mp_face_mesh = mp.solutions.face_mesh

with mp_face_mesh.FaceMesh() as face_mesh:
    res = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    for face_landmarks in res.multi_face_landmarks:
        pass
```

åˆ©ç”¨ CV2 æ•è·è§†é¢‘ï¼š

```python
import cv2
import mediapipe as mp

mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_face_mesh = mp.solutions.face_mesh

drawing_spec = mp_drawing.DrawingSpec(thickness = 1, circle_radius = 1)
cap = cv2.VideoCapture(0)

with mp_face_mesh.FaceMesh(
    max_num_faces = 1,
    refine_landmarks = True,
    min_detection_confidence = 0.5,
    min_tracking_confidence = 0.5
) as face_mesh:
    while cap.isOpened():
        success, image = cap.read()
        if not success:
            print("Ignoring empty camera frame")
            continue
        
        # To improve performance, optionally mark the image as not writeable to pass by reference.
        image.flags.writable = False
        image = cv2.cvtColor(image, cv2.COLOR_BGR2BGR)
        results = face_mesh.process(image)
        
        
        
        # Draw the face mesh annotations on the image.
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        if results.multi_face_landmarks:
            for face_landmarks in results.multi_face_landmarks:
                mp_drawing.draw_landmarks(
                    image=image,
                    landmark_list=face_landmarks,
                    connections=mp_face_mesh.FACEMESH_TESSELATION,
                    landmark_drawing_spec=None,
                    connection_drawing_spec=mp_drawing_styles
                    .get_default_face_mesh_tesselation_style())
                mp_drawing.draw_landmarks(
                    image=image,
                    landmark_list=face_landmarks,
                    connections=mp_face_mesh.FACEMESH_CONTOURS,
                    landmark_drawing_spec=None,
                    connection_drawing_spec=mp_drawing_styles
                    .get_default_face_mesh_contours_style())
                mp_drawing.draw_landmarks(
                    image=image,
                    landmark_list=face_landmarks,
                    connections=mp_face_mesh.FACEMESH_IRISES,
                    landmark_drawing_spec=None,
                    connection_drawing_spec=mp_drawing_styles
                    .get_default_face_mesh_iris_connections_style())
                
        # Flip the image horizontally for a selfie-view display.
        cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))
        if cv2.waitKey(5) & 0xFF == 27:
            break
cap.release()
```






### ç¤ºä¾‹3

å‚è€ƒ: [å®˜æ–¹ä»£ç ](https://github.com/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb)

ä¸ç”¨ mp.solutions äº† ç”¨ mp.tasks

**3ä¸ªé‡è¦åŠŸèƒ½ï¼š**
- é¢éƒ¨ç½‘æ ¼
- å½¢çŠ¶ç‰¹å¾
- transformation matrix


```python
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
import matplotlib.pyplot as plt

mp_drawing = mp.solutions.drawing_utils

def draw_landmarks(rgb_image, res):
    annotated_image = rgb_image.copy()
    for face_landmarks in res.face_landmarks:
        
        # åšä¸€æ¬¡ç‚¹ä½è½¬æ¢ï¼Œä½œç”¨ï¼Ÿ
        face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
        face_landmarks_proto.landmark.extend([
            landmark_pb2.NormalizedLandmark(x = landmark.x, y = landmark.y, z = landmark.z) for landmark in face_landmarks
        ])
        
        mp_drawing.draw_landmarks(
            image = annotated_image,
            landmark_list = face_landmarks_proto,
            connections = mp.solutions.face_mesh.FACEMESH_TESSELATION,
            landmark_drawing_spec = None,
            connection_drawing_spec = mp.solutions.drawing_styles.get_default_face_mesh_tesselation_style())
        mp_drawing.draw_landmarks(
            image = annotated_image,
            landmark_list = face_landmarks_proto,
            connections = mp.solutions.face_mesh.FACEMESH_CONTOURS,
            landmark_drawing_spec = None,
            connection_drawing_spec = mp.solutions.drawing_styles.get_default_face_mesh_contours_style())
        mp_drawing.draw_landmarks(
            image = annotated_image,
            landmark_list = face_landmarks_proto,
            connections = mp.solutions.face_mesh.FACEMESH_IRISES,
            landmark_drawing_spec = None,
            connection_drawing_spec = mp.solutions.drawing_styles.get_default_face_mesh_iris_connections_style())

    return annotated_image


# plot_face_blendshapes_bar_graph
def show_feature(bs):
    names = [i.category_name for i in bs]
    scores = [i.score for i in bs]
    ids = range(len(names))
    
    fig, ax = plt.subplots(figsize=(12, 12))
    bar = ax.barh(ids, scores, label = [str(i) for i in ids])
    ax.set_yticks(ids, names)
    ax.invert_yaxis()
    
    for score,patch in zip(scores, bar.patches):
        plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f"{score:.4f}", va="top")

    ax.set_xlabel('Score')
    ax.set_title("Face Blendshapes")
    plt.tight_layout()
    plt.show()
    
    

base_options = python.BaseOptions(model_asset_path = 'face_landmarker_v2_with_blendshapes.task')
options = vision.FaceLandmarkerOptions(base_options = base_options,
                                      output_face_blendshapes = True,
                                      output_facial_transformation_matrixes = True,
                                      num_faces = 1)

detector = vision.FaceLandmarker.create_from_options(options)

# ä½¿ç”¨ 4 é€šé“çš„pngä¼šæŠ¥é”™
image = mp.Image.create_from_file("person2jpg.jpg")
res = detector.detect(image)

# åŠŸèƒ½1ï¼š é¢éƒ¨ç½‘æ ¼
annotated_image = draw_landmarks(image.numpy_view(), res)
plt.imshow(annotated_image)

# åŠŸèƒ½2ï¼šå½¢çŠ¶ç‰¹å¾ (52 ä¸ªé¢éƒ¨ç‰¹å¾)
show_feature(res.face_blendshapes[0])

# åŠŸèƒ½3ï¼štransformation matrix
print(res.facial_transformation_matrixes)
```


ä½¿ç”¨ é€šé“ä¸ç¬¦çš„ png å¯èƒ½æŠ¥é”™ï¼Œè½¬åŒ–ä¸€ä¸‹ï¼š

```python
from PIL import Image

img = Image.open("person_head.png")
img = img.convert('RGB') 
img.save("person2jpg.jpg")
```

### ç¤ºä¾‹4 ğŸ”

ä½¿ç”¨ mp.tasksï¼Œå‚è€ƒ [å¼€å‘è€…æ–‡æ¡£](https://developers.google.com/mediapipe/solutions/vision/face_landmarker/python)

éœ€è¦å…ˆä¸‹è½½æ¨¡å‹æ–‡ä»¶

```python
import mediapipe as mp
from pprint import pprint

BaseOptions = mp.tasks.BaseOptions
FaceLandmarker = mp.tasks.vision.FaceLandmarker
FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions
VisionRunningMode = mp.tasks.vision.RunningMode

options = FaceLandmarkerOptions(
    base_options=BaseOptions(model_asset_path=model_path),
    output_face_blendshapes=True,                # é»˜è®¤ä¸º False
    output_facial_transformation_matrixes=True,  # é»˜è®¤ä¸º False
    running_mode=VisionRunningMode.IMAGE)


# Load the input image from an image file.
mp_image = mp.Image.create_from_file('image.jpg')

# Load the input image from a numpy array.
# mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=numpy_image)
    
with FaceLandmarker.create_from_options(options) as landmarker:
    res = landmarker.detect(mp_image)
    # print(res,type(res))
    # print(res.face_landmarks)
    pprint(res.face_blendshapes)
    # print(res.facial_transformation_matrixes)


# æ‹¿åˆ° res å°±ä¸å¿…å—é™äº†ï¼Œç›´æ¥å–å°±è¡Œ
# å¦‚ res.face_landmarks[0] ï¼Œä¹Ÿä¸å¿…åƒç¤ºä¾‹3 ä¸€æ · Normalize(æå‰åšäº†)
```

æ›´å¤šçš„å–æ³•ï¼š

```python
res.face_landmarks[0]       #ç¬¬ä¸€ä¸ªäºº
res.face_landmarks[0][0]    #ç¬¬ä¸€ä¸ªäººç¬¬ä¸€ä¸ªç‚¹ä½
res.face_landmarks[0][0].x

[i.category_name for i in res.face_blendshapes[0]]  # æŸ¥çœ‹ 52 ä¸ªé¢éƒ¨ç‰¹å¾ï¼Œçš„åå­—

# res.face_blendshapes[0]: 
#       index 0~51
#       socre 0~1
#       category_name
```






> æ–‡æ¡£å†™å¾—ä¸€å¨æµ†ç³Šï¼Œmediapipe.tasks ä¸ mediapipe.solutions ä¸¤å¥—ä»£ç å¿äº†ï¼Œå®˜æ–¹åŒä¸€ä¸ªç¤ºä¾‹ä¸­è¿˜æ•´ mediapipe.tasks.python.vision å’Œ mediapipe.tasks.vision ä¸¤å¥—



### ç¤ºä¾‹5

çœ‹çœ‹å…¶ä»– vtuber çš„ä»£ç æ˜¯æ€ä¹ˆç”¨mediapipe çš„





## Hand landmarks

å‚è€ƒï¼š[colab ä»£ç ](https://colab.research.google.com/drive/1FvH5eTiZqayZBOHZsFm-i7D-JvoB9DVz)


```python
import mediapipe as mp
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
# help(mp_hands.Hands)

images = {name: cv2.imread(name) for name in ['hand.png']}

with mp_hands.Hands(
    static_image_mode = True,
    max_num_hands = 2,
    min_detection_confidence = 0.7) as hands:

    for name, image in images.items():
        res = hands.process(cv2.flip(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1))

        print(f"Handedness of {name}:")
        print(res.multi_handedness)  # æœ‰ä¸ªåˆ†ç±»å™¨ï¼Œä¼šåˆ†å‡ºå·¦å³æ‰‹

        if not res.multi_hand_landmarks:
            continue

        print(f"Hand landmarks of {name}")
        image_hight, image_width, _ = image.shape
        annotated_image = cv2.flip(image.copy(), 1)

        for hand_landmarks in res.multi_hand_landmarks:
            # è¿™æ ·å­å–ç‚¹ .landmark[].xï¼Œå¹¶ä¸”mediapipe æ˜¯å¤„ç†æˆæ¯”ä¾‹ç‚¹ï¼Œæœ€å¤§ä¸º1ï¼Œ éœ€è¦ *image_width
            print(
                f'Index finger tip coordinate ',
                f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width},',
                f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_hight}'
            )
            mp_drawing.draw_landmarks(
              annotated_image,
              hand_landmarks,
              mp_hands.HAND_CONNECTIONS,
              mp_drawing_styles.get_default_hand_landmarks_style(),
              mp_drawing_styles.get_default_hand_connections_style())
        resize_and_show(cv2.flip(annotated_image, 1))
```

?> `cv2.flip(src, flipCode[, dst])` å°†å›¾ç‰‡è¿›è¡Œç¿»è½¬ï¼Œ 0 æ°´å¹³ç¿»è½¬ï¼ˆæ²¿ y æ–¹å‘ï¼‰ï¼Œ1 å‚ç›´ç¿»è½¬ï¼Œ-1 åŒæ—¶ç¿»è½¬


å¯ä»¥åœ¨ 3d ç©ºé—´ç»˜åˆ¶ï¼Œç‰›çš„ï¼Œä» 2d å›¾ç‰‡ä¸­å°±å¯ä»¥æå‡º 3d çš„ä¿¡æ¯

```python
with mp_hands.Hands(
    static_image_mode=True,
    max_num_hands=2,
    min_detection_confidence=0.7) as hands:
    
    for name, image in images.items():
        res = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

        print(f'Hand world landmarks of {name}:')
        if not res.multi_hand_world_landmarks:
            continue

        for hand_world_landmarks in res.multi_hand_world_landmarks:
            mp_drawing.plot_landmarks(hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)
```







å‚è€ƒï¼šhttps://cloud.tencent.com/developer/article/2081312


```python
import mediapipe as mp

mpHands = mp.solutions.hands
hands = mpHands.Hands()

imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
res = hands.process(imgRGB)

if res.multi_hand_landmarks:
    for handLms in res.multi_hand_landmarks:
        pass

```




## Pose landmark

å‚è€ƒï¼š [å®˜æ–¹æ–‡æ¡£](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker) [github æ–‡æ¡£](https://github.com/google/mediapipe/blob/master/docs/solutions/pose.md)



```python
import cv2
import mediapipe as mp
import numpy as np
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_pose = mp.solutions.pose

# For static images:
IMAGE_FILES = []
BG_COLOR = (192, 192, 192) # gray
with mp_pose.Pose(
    static_image_mode=True,
    model_complexity=2,
    enable_segmentation=True,
    min_detection_confidence=0.5) as pose:
  for idx, file in enumerate(IMAGE_FILES):
    image = cv2.imread(file)
    image_height, image_width, _ = image.shape
    # Convert the BGR image to RGB before processing.
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    if not results.pose_landmarks:
      continue
    print(
        f'Nose coordinates: ('
        f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].x * image_width}, '
        f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.NOSE].y * image_height})'
    )

    annotated_image = image.copy()
    # Draw segmentation on the image.
    # To improve segmentation around boundaries, consider applying a joint
    # bilateral filter to "results.segmentation_mask" with "image".
    condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1
    bg_image = np.zeros(image.shape, dtype=np.uint8)
    bg_image[:] = BG_COLOR
    annotated_image = np.where(condition, annotated_image, bg_image)
    # Draw pose landmarks on the image.
    mp_drawing.draw_landmarks(
        annotated_image,
        results.pose_landmarks,
        mp_pose.POSE_CONNECTIONS,
        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())
    cv2.imwrite('/tmp/annotated_image' + str(idx) + '.png', annotated_image)
    # Plot pose world landmarks.
    mp_drawing.plot_landmarks(
        results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)
```











## Holistic landmarks


> ä¸€ä¸ªé›†æˆçš„æ¨¡å—ï¼Œå§¿æ€æ£€æµ‹ + æ‰‹éƒ¨æ£€æµ‹ + é¢éƒ¨æ£€æµ‹ + ç´ æåˆ‡å›¾

å‚è€ƒï¼š[å®˜æ–¹colab](https://colab.research.google.com/drive/16UOYQ9hPM6L5tkq7oQBl1ULJ8xuK5Lae)


```python
import mediapipe as mp
import matplotlib.pyplot as plt


mp_holistic = mp.solutions.holistic
# help(mp_holistic)

mp_drawing = mp.solutions.drawing_utils 
mp_drawing_styles = mp.solutions.drawing_styles

images = {name: cv2.imread(name) for name in ["pose.png"]}


with mp_holistic.Holistic(
    static_image_mode = True,
    min_detection_confidence = 0.5,
    model_complexity = 2) as holistic:
    
    for name, image in images.items():
        res = holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        image_high, image_width, _ = image.shape
        
        print(
            f'Nose coordinates: ('
            f'{res.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].x * image_width}, '
            f'{res.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].y * image_hight})'
        )

        print(f'Pose landmarks of {name}:')
        
        annotated_image = image.copy()
        mp_drawing.draw_landmarks(annotated_image, res.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
        mp_drawing.draw_landmarks(annotated_image, res.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
        mp_drawing.draw_landmarks(
            annotated_image,
            res.face_landmarks,
            mp_holistic.FACEMESH_TESSELATION,
            landmark_drawing_spec=None,
            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())
        mp_drawing.draw_landmarks(
            annotated_image,
            res.pose_landmarks,
            mp_holistic.POSE_CONNECTIONS,
            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())

        resize_and_show(annotated_image)
```

åŒæ ·åœ°ï¼Œå¯ä»¥åœ¨ 3D ç©ºé—´ä¸­ç»˜åˆ¶è¡¨ç¤º

```python
# Run MediaPipe Holistic and plot 3d pose world landmarks.
with  mp_holistic.Holistic(static_image_mode=True) as holistic:
    for name, image in images.items():
        results = holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

        # Print the real-world 3D coordinates of nose in meters with the origin at
        # the center between hips.
        print('Nose world landmark:'),
        print(results.pose_world_landmarks.landmark[mp_holistic.PoseLandmark.NOSE])

        # Plot pose world landmarks.
        print(f'Pose world landmarks of {name}:')
        mp_drawing.plot_landmarks(
            results.pose_world_landmarks, mp_holistic.POSE_CONNECTIONS)
```

## è®ºæ–‡é˜…è¯»


[Attention Mesh: High-fidelity Face Mesh Prediction in Real-time](https://arxiv.org/pdf/2006.10962.pdf)

å…ˆæ˜¯ç»è¿‡ Blaze Face Detector äººè„¸æ£€æµ‹æ¨¡å‹ï¼Œä¹‹åå¯¹ç‰¹å¾åŒºåŸŸæå–å±€éƒ¨ç‰¹å¾ã€‚

æ³¨æ„åŠ›æœºåˆ¶å…·ä½“çš„åšæ³•ä¸ºï¼šåœ¨ç‰¹å¾ç©ºé—´ä¸Šè®¾ç½®äºŒç»´ç½‘çŠ¶é‡‡æ ·ç‚¹ï¼Œå¹¶ä½¿ç”¨äºŒç»´é«˜æ–¯æ ¸æˆ–è€…ä»¿å°„å˜æ¢ä¸å¾®åˆ†å·®å€¼ï¼ˆdifferen-tiable interpolationsï¼‰åœ¨é‡‡æ ·ç‚¹å¯¹åº”çš„ç‰¹å¾å›¾ä¸Šæå–ç‰¹ç‰¹å¾ã€‚



---------------

å‚è€ƒèµ„æ–™ï¼š
- https://yinguobing.com/attention-mesh-as-face-mesh-solution-from-google/






## other


MediaPipe Iris ä¸“é—¨ç”¨äºè™¹è†œï¼Œèƒ½å¤Ÿè®¡ç®—å‡ºæ·±åº¦ç­‰ï¼ˆåœ¨å¯å˜å…‰ç…§ã€é®æŒ¡ç‰©ç­‰æƒ…å†µï¼‰

[åº”ç”¨ï¼šäº¤äº’æŠ å›¾](https://mediapipe-studio.webapps.google.com/demo/interactive_segmenter)

æä¾›äº†ä¸€ä¸ªæ€è·¯ï¼šå¦‚æœåç»­æŠ å›¾ä¸å¤Ÿå¹²å‡€çš„æ—¶å€™ï¼Œå¯ä»¥é‡‡ç”¨è¿™ä¸ªæ–¹å¼ç»†è‡´æŠ å›¾ã€‚

æˆ–è€…æŠŠè¿™ä¸ªéƒ¨ç½²åˆ°ç½‘é¡µï¼Œæˆ–è€…æ›´æ¢æ¨¡å‹




