
任务描述：实现视频换脸等特效

</br>

## _1. 视频直接转视频_

方案一：VToonify

优点：能保证前后帧的连贯性，效果一般

缺点：模型比较难训（先是训 特殊的风格转化模型，之后再训 生成视频的模型），且风格较固定


</br>


方案二：抖音 迪士尼特效，参考 https://www.easemob.com/news/9937
- 原始风格训练 StyleGAN 大模型，人工挑选成对数据，p 图等，训练 p2p 小模型
- 不断反复，迭代周期长
- 美术素材，DCC 软件批量渲染 CG 数据，算法合成 GAN 成对数据，训练 GAN



</br>

## _2. 视频切帧 -> 逐帧风格转化 -> 合成视频_


方案一：stable-diffusion 

优点：sd社区优秀

缺点：从 https://www.ithome.com/0/680/699.htm 这篇文章来看，工作量较大，暂时不考虑

（sd 如何生成可控的动画：给 AI 锁定了动画的风格，同时也要锁定演员的样貌，多角度构造同一角色的素材）（之后用上视频特效技术减少闪烁）。（前后帧可能不连贯，且生成具有随机性）



</br>

方案二：DCT-Net

优点：素材内容小幅度变化时效果可以，容易使用

缺点：素材出现大幅度动作时效果不好。训练较难，需要配对数据。（前后帧可能不连贯）



</br>

## _3. 风格转化，只生成其中的关键帧，AI自动补帧_

优点：理论上能保证连贯性。

缺点：但素材中，说话的场景十分多，可能嘴部无法很好拟合。暂未调研，工作量较大

[模型1](https://github.com/OndrejTexler/Few-Shot-Patch-Based-Training)     EbSynth


</br>

## _4. 真人换真人换脸_

优点：有着相对较多的开源模型

缺点：头发没办法更换



!> 上面 4 种方案，都是转化的路线，完全依赖神经网络黑盒。


</br>

## _5. 人脸关键点检测 + 卡通关键点对齐 + 跟踪贴图_


对齐方式：
- 选取三个点唯一确定仿射变换，利用 opencv 实现
- 3D 模型实现（未调研）

优点：可以实现贴图直接覆盖掉原来的图像区域

缺点：比较依赖素材场景和贴图素材（需要准备同一形象的各种动作的素材）

基于关键点检测，计算出特征，从素材库（同一卡通形象各种动作，如嘴巴张合，头部偏向）中选取，之后使用 仿射变换 来完成关键点对齐



</br>

## _6. 视频动捕，vtuber 通用方案_

比较可行，

参考左侧，视频动捕-方案概览


</br>

## _7. 视频驱动图片进行运动_


使用模型： first-order-model

优点：不需要什么素材

缺点：分辨率很低、空间扭曲严重



</br>

## _8. 驱动 3D 模型_


（1）如苹果的animoji，比较 low，也没好看的 3D 模型给用.

（2）发现一个较厉害的网站（强需3D模型，）：https://wonderdynamics.com/

有些瑕疵

（1：估计抠图不太干净，导致有些黑影）

（2：模型卡模，涉及遮挡性动作时），能做成这样算不错了，估计走的 姿态检测+3d驱动 路线，。分析镜头？


