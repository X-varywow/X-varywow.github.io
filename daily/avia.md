
torchvision.utils.make_grid(F.adaptive_avg_pool2d(torch.cat([img_rec, img_gen, img_gen2], dim=0), 256), 4, 1)

https://www.techxiaofei.com/post/ai/cartoon/


- [x] 了解业务新需求：数字资产管理系统&项目管理系统
  - 简单实现思路：（库为s3, 本地仿照git,总共三个地点）（这不写个streamlit 打通本地与s3, 文件管理系统就行？）（网页可行就行，app没太必要？）(后续计划：自动化，打通飞书,浏览器插件，各种数据格式预览，显示子文件夹，版本控制，人工智能识别标签，东西真多，要简单也能简单，许多冗余了)（就是软件开发，工程化知识很多，）（做个共享导航页面）（主要是需要前端人员？）
  - 核心：降本增效



`8.02`

aws s3 sync s3://dataai-warehouse-stages/machine-learning/matchmaking/hzx_models/tmp_audio_service /home/ec2-user/SageMaker/vitssvc

有留档真好啊，多多备份。一定要多多备份。


- [x] 重新部署 audio-service，解决 bug
- [x] 图搜视频整体流程跑通，功能测试，性能测试完成，等业务方沟通对接方式可以进行下一步(clip + milvus)(代码方面，boto3 s3和sagmaker 连通，切帧、clip 提取特征、建议gradio等)（缺点：只考虑图片帧信息，没有考虑视频连续性附带的信息，需要时序网络？）（CLIP 还有优点，可根据 text 直接匹配）
- [ ] 分析 audio 代码，让模型常驻手动卸载



`8.07 ~ 8.11`

目标：需要准备 live2d 素材（后期也可考虑3D）以及原视频，自动替换视频中人物（可选头部）


- [x] 跑通 live2d web SDK
- [ ] 修改 kalidokit 网页，使视频可以放进去改变（试试吧，有点难，且 kalidokit 这个库已经被集成了）
- [ ] 修改 google 网页，改不动
- [ ] 自己动手，查看变换方式

live2d\mediapipe 分别是两端，进行不下去可以看一下


目前路线，live2d 跟网页连接，再与 python 连（可选），不然 live2d 生成动画需要专用软件。熟悉前端。


- [ ] 把简单的视频跑通，没添加远近（应该不改）
- [ ] 修改 3web 代码，添加更多的自由度，动作幅度，更换模型
- [ ] 如何查看 live2d 配置项的取值范围


- [x] 确定要使用的技术栈，mediapipe + python + unity
- [x] 简单修改代码，配置项理解没错；通过 coreModel.setParameterValueById来设置配置项的变形，这个变形是利用变形器实现的区域性变形，但是在外部就用数字表示了。



`console.log(JSON.stringify(res))`,查看了 kalidokit 返回的结构，查看 mediapipe 新的结构或手动添加新的特征结构



`0808`

- [x] mediapipe 熟悉，大概有 4个要用到
- [x] mediapipe colab face\mesh\hand\pose 都码一遍，熟悉
- [x] mediapipe 表情特征，熟悉
- [x] 很熟悉了，会提取就行， 478 个点， 52 个特征

扫一下文档，就去 unity 跑 demo 了；

python 端已经可以：熟练使用 mediapipe，提取视频中的点位信息，特征信息，方便后续传递给 unity


`0809`

居然能跑 unity...

主线：复现并优化 vtuber-python-unity

- [x] python 端跑通，作者牛的
- [x] unity sdk 跑通
- [x] unity 载入模型，更新配置项； prefab 的形式
- [x] 跟[视频](https://www.youtube.com/watch?v=3pBc9Wkzzos&t=30s)学习，完成demo (导包就行。。。)
- [ ] 能否添加更多的自由度，并添加放缩自由度，
- [ ] 后续使用视频做输入输出


https://www.youtube.com/watch?v=NYuAU4QUjB8 作者

unity 通的, web 通的。简单处理下视频，生成动画。


如果 pythonforunity 可以的话，应用什么的都可以用 unity 来做

python 处理，数据回传，unity 生成，导出？





`0810`

先不改代码，生成一个 demo 视频。
- [x] 把视频作为输入，传出的信息弄成 文本保存
- [x] 检查 468 -》 478，绘制瞳孔点位

这个 Kalman filter as a point stabilizer. 也需要一段时间理解，后续


先做面部替换，姿态、手势等不好做，且需要弄个新的模型


人脸检测比 opencv 好用，不会空帧




`0811`


修改 python 端代码，利用 opencv 修改输入，把摄像头捕获的方式改为 视频输入，视频分帧，弄了个暂停方便调试，并将计算的特征保存到 json。

查看代码，作者只计算了 11 个特征， 涉及相机姿态估计、卡尔曼滤波，尝试修改特征的计算，暂时只对面部特征计算。有个计算有些问题，导致不稳定。

unity electron 做底座，集成 python socket 对接


`0815`

旧的 mac 不支持新方法，看了绝大部分开源的视频动捕，基本都是虚拟主播这一个应用，大部分都是使用 unity 开发应用界面的，目前已经可以视频动捕了，利用 python 跑 mediapipe 的 soltion 旧方法 + 手动计算特征，并传递给 unity 驱动 live2d 模型，但存在很多细节问题：动作存在抖动，特征计算方式存在问题，不能很好映射。

所以接下来，换 mac 系统（暂时方法）或 unity mediapipe 插件，unity 集成之后纯用 unity 实现。（python mediapipe 旧方法没）

由于面捕驱动的自由度还是有限，google 有个[示例](https://mediapipe-studio.webapps.google.com/studio/demo/face_landmarker)能很好利用 3D 模型捕捉到面部肌肉变化，但没有具体实现。（webgl 基于 facelandmark，爬bu）
// 其实除了体现肌肉变化外，不太好，越界等
// 需要入微地了解变形器机制，或者每个区块单独做一个仿射变换。（2D 和 3D 又有区别，达到广告效果，其实二者都靠质量）
// 存在闪烁的问题，但不存在动作不连贯的问题，动捕技术本身就是专门解决这个问题的。

所以暂时及最终：使用 unity 开发一款软件，使用摄像头或视频作为输入源，右边放模型，可驱动，可导出，暂时把这个工具做好，之后继续改进和优化。很长的路啊。

**最终是一个 unity应用**，开销没太大问题，因为这种姿态估计任务，已经比较成熟了。但整套方案不太成熟，驱动起来细节挺多。


- [x] mediapipe 0.9 m 不行啊
- [x] Stabilizer 细节
- [x] unity + mediapipe 更多代码细节


`0816`

- 测试软件，3web, aklidokit,
  - （kalidoface 完全打不开，95% 是 web + electron 做的）（以后做工具的时候可以参考，）
  - （kalidoface web 版确实好看，作者是个 web 人）（直接报错，找不到 facemesh + holistic）(带有远近功能)
  - https://3d.kalidoface.com/ 可以访问
  - glitch 弄下来
- VRM
  - 2019 就有相关的文章了
- 3web 
  - 可以借鉴眼睛的计算方式
  - 没使用卡尔曼，也没有不平稳，不知道哪个 uni-mp1 怎么计算的，坑
  - （太稳了，使用的是中间值，看起来过渡非常自然。）


通过帧操作，包括 unity 中也是保存帧，来确保完全的对齐。

按道理如果使用最新的 mediapipe for web，效果应该不如 3web 的效果好


- [x] 3web 下熟悉 web 调用 mp solution
- [x] 3web 跑通
- [x] 查看代码，箭头函数弄清，其实没啥难理解的
- [ ] 参考官方文档添加按键动画


这欧拉角的计算，数学，，，

mp holistic solution 更新方式：camear.start() - > holistic.send() -> holistic.onResults(res) 不断触发来更新页面绘图 


`0818`

0818 视频动捕代码开发，近期工作小结：

- 视频动捕代码开发
  - 开发了简易的 unity 使用界面
  - 使用 python 拼接视频，最终效果如下

- 应用
  - 应用1：将动捕生成的动画，只取人物头像区域，替换素材中人物头像。（效果如下）
  - 应用2：将动捕生成的动画，
  - 应用3：完全使用动捕生成的动画，素材只作为生成动画的参考。最终使用


- （详细流程）：
  - 基于 opencv 和 mediapipe（python SDK）, 实现对视频的人脸表情捕捉。（同时，测试了姿态、手势捕捉，目前没应用上去）（实际运行&调试效果如下）
  - 将捕捉到的人脸3D点位信息，通过 相机姿态估计、卡尔曼滤波稳定抖动、特征计算（眼嘴闭合计算、偏移角度计算等）等计算出 live2d 角色限定的配置项
  - 使用 unity 作为程序界面和渲染引擎，通过本地 socket 接收 python 传递过来的数据项（实际运行&调试效果如下）
  - 使用 live2d 作为驱动的模型（2d的，后续也可以变成驱动3D 模型，目前没找到比较细腻的3D 模型）
  - 保存渲染出来的视频，再使用 python 实现视频与原素材的拼接（批量抠图、遮罩、连接音轨等）



`0821`

周末：
- 测评了抖音， 基本几类特效：滤镜美化类，贴图特效类，卡通形变类（迪士尼独一党），人脸贴3D 类。
- 完全机器学习（使用生成模型，SD, GAN 等）几乎不太可能，模型是有局限性的，素材需要同一风格，且需要朝向不同素材，并建立真实人物的绑定关系，还要保证视频前后帧的连贯，很难。
- 比如动物脸，完全的机器学习，根本没思路。（现实中也没看见能做的公司）


更换电脑，讨论思路：3D 动捕并替换


`0822`

- [x] 整理 avia
- [x] blog - 方案汇总，总结换脸方案
- [x] blog - 0822 vtuber
- [x] kalidoface vrm 版本

- [x] sysmocap 测评
- [x] electron
- [x] 阅读 sysmocap 源码
- [x] 调研头部方案：头部抠图（自动生成蒙版，参考）完全覆盖


- [ ] 编辑 3D 文件，只生成头部；学一下基础的 3D 建模
  - [ ] blender
  - [ ] vroid studio (vrm 可以很细腻，更适合本场景)

- [ ] 先用最普通的 vrm 套上去看一下效果

- [ ] 调研 https://docs.omniverse.nvidia.com/nucleus/latest/enterprise/cloud_aws_vdi.html
- [ ] 调研方案，3D 动捕，或者更好地2D ，
- [ ] unity, c#
- [ ] https://zhuanlan.zhihu.com/p/58850085 搜索算法
- [ ] CG 渲染数据



- 开销问题
  - unity 开销 和 web 开销， web 会好些？ chatgpt
  - 但是 electron 会打包整个 chromium 浏览器引擎，需要同时运行浏览器引擎和 nodejs。性能开销会更大
- 头部拼接
  - 获取 3D 模型头部，并调整大小、表情特征、角度等，与原素材匹配
  - 图像叠加、蒙版、透明度，将二者合并
    - 完全覆盖实现拼接
    - 自动填充式抠图，实现拼接
  - 优化拼接效果，颜色矫正、透明度
- 最终形式？
  - electron or unity 都行
  - 能学 electron, hhh




`0823`

昨天
1. 测评软件，查看效果，代码， 并不好，只能8帧
2. 头部替换方法，完全覆盖（可用可行，面部不能遮挡），抠图贴图（一般全身、鲁棒性差）



希望目标（待对齐），实时的换头摄影（替代抖音的摄影）工具。

可能用到的模型：
- [视频人像抠图](https://modelscope.cn/models/damo/cv_effnetv2_video-human-matting/summary)
- [LaMa图像填充](https://modelscope.cn/models/damo/cv_fft_inpainting_lama/summary)
- 视频描述


如果能实现这种效果：https://www.douyin.com/zhuanti/7186533044424099840 ，再自制 3D 头套模型，那应该能达到效果。


识别 -> 驱动 -> 对齐 -> 图像处理


- [x] 3d 知识，只保留头部，合理拼接尝试，阅读文档调研 的一天
- [x] 分析 web、还是 unity 实现？ 什么样数据格式
- [ ] 获取几个 3D 头部模型


`0824`

面部肌肉不可能完全匹配，造成比如远近移动、朝向移动时，点的移动距离会是不同的，而3D 模型（牛头）一般不会这么生动的形变。一般都是选择几个点做几何变换，这种变换直接导致了遮住了脖子。

且，虚拟头套与真人脖子本来就是明显差异的，就算不遮也会看出明显差异。且，涉及到相机视角问题，真人脖子也会被遮住了

所以：遮住脖子是较好的选择

0930
- [x] 离职交接文档

下午
- [x] 业务方沟通，同步结果
- [x] 神经网络, 整理 ML



`0825`


- [x] 回消息
- [x] 调研下面几个纯神经网络方案
- [ ] vtoonify

vt 对四周基本不进行风格转化


## [StyleAvatar3D](https://github.com/icoz69/StyleAvatar3D)

使用sd 进行数据生成，并利用 GAN 进行 3D 的训练

- 利用图像文本扩散模型提供的外观和几何的综合先验来生成各种风格的化身的多视图图像。
- 数据生成过程中，我们采用从现有 3D 模型中提取的姿势来指导多视图图像的生成。
- 解决数据中的姿势和图像之间的错位问题，我们研究了特定于视图的提示，并为 GAN 训练开发了从粗到细的判别器。
- 深入研究了与属性相关的提示，以增加生成的头像的多样性。
- 在 StyleGAN 的风格空间内开发了一个潜在扩散模型，以便能够根据图像输入生成头像。

只是特定内容的生成


`0828`

重点：视频人像卡通化，，，其实生成图片还算简单，视频限制性太多了

优化 VToonify （试过很多次，没底）(代码上优化？)

sd 也太闪烁了，，

调研了一堆直接视频转的，

- [x] stablevideo 真实视频尝试，，，今天就部不上去，一直在 load config；算了
- [x] roop 试试效果，，，（30s 视频 9min）（贴合有瑕疵，但总体效果不错）（估计是前发遮挡造成的，）
- [x] google 网页获取（有团队真舒服。）并修改
- [ ] roop 部署 sagemaker
- [ ] roop 测试更多素材



> 直接换脸就可以做到了，不必整那么复杂...  补充一下理论，找点素材开始训就行。

头发也能更换，当时怎么没多花时间在这个方向。绕了 vtoonify, 视频处理，动捕unity 一大圈，还好。


走换脸了，guoba 一大堆网络真多啊，这么多水论文，应用场景不明的论文，，，




`0829`

与业务方对齐，近期工作小结。

- 调研方案等
- (1)换脸路线
  - faceswap
- (2)转化路线
  - vt
  - DCT-Net
- (3)动捕换脸
  - 3D 仿照 google


- [x] colab 跑 roop
- [x] 资料整理
- [x] roop 部署
- [x] roop 出个初步的结果 （colab 好好的, sagemaker人脸都检测不到，部分卡通可检测）
- [x] ffmpeg 不能保存问题 ( -crf 参数取消)
- [ ] 查看源代码，更改检测网络，使其适应卡通脸
- [ ] 查看原理
- [ ] 能否走 faceswap
- [x] ghost 论文

估计最后还是要走faceswap


就是缺素材，



`0830`

- [x] colab facefusion测试 （太难了，，）
- [x] sagemaker 修 bug
- [x] mac 本地跑 facefusion （卡住）
- [x] deepfacelab 论文
- [ ] 补理论了，回去找素材训练
- [ ] win DFL 开始训练
- [ ] win facefusion


另一种思路：脸部变形使其适应卡通脸，再通过换脸实现完全的卡通脸。

分析disney 脖子部分的扭曲：大概率是变形美颜的原因，不是神经网络风格转化的锅，



`0831`


sd-webui-ai 卡通脸

变形等前修正 + 换脸 + 超分等后修正



- [x] FOMM 算了， ffmpeg 又是出问题，昨天
- [ ] 回消息
- [ ] 补充理论，controlnet, lora


这样下去大概率是不行的，

DFL 需要多角度素材，估计也不能提供，

所以能做的只是：**在原本素材的基础上应用些变形**，例如 vtoonify，codef



`0901`

讲 lora, control net

使用说明：基于 facefusion（修改检测使其适应卡通， 修改 UI ） + 人脸变形 + ...



`0904`

（1）方案整体性理解

迪士尼特效：脖子连着头部都是拼接上去的，模型
头部可以遮挡，这就离谱了。猜测：分割出不同图层，然后更换头部

snapchat 也类似，但是脸部存在抖动，（就是头部抠图到脖子，然后转换后拼上去）新特性：编辑脸部大小，

DCTNET，关键是 3D 风格不明显，需要准备大量数据


（2）计划方案，
动捕一种，头套没遮住全部头发没事
分层抠图，再风格转换一种，（DCT-Net）优化


（3）问题评价

最终目标导向不了，不能变更（不然可以不用真人驱动，语音驱动图生视频，或制作动画）。

**二级目标导向：真人原素材（或 disney转化），转换为目标视频（卡通，动物）**，二级目标好难。。

三级目标：优化卡通脸效果（例如，肤色、嘴唇厚度）

很离谱，分图分不干净，走转化一般只能走全局的转化， vtoonify 取巧选择了一个框框，且四周差异并不大，才能这样做。


这剪映卡通效果也太抽象了，鼓鼓囊囊的，大团队才做成这样；iphone卡通头像，贴图式，也十分抽象。。。

snapchat 动漫换脸也是抽象，就是风格转化，前后之间闪个不停

vtoonify 炼丹？


（4）TODO

排除吧，太难了。。。

- 换脸，需要脸型对上，**训一版或找到卡通效果可以定论** `不行` 网络上找不到卡通换脸，【十分抽象，这套方案就不是为了卡通脸设计的，原理不搭】
- 动捕，**需要3D模型进行下一步**，找市场测试素材效果。目前只有个浣熊头
- VToonify，实在没办法弄这个，以前训了很多版，效果并不好 `暂时不弄`
- codef 等偏门，`不弄` （变化幅度受限，cv理论较多）


走动捕 + 修复脖子，是最理论可行的路线了，后期可以再加上分图和图层；理论上不会恐怖谷
**弄多一点3D头套，挺有趣的。**


- [x] 找到换脸的卡通效果，只试过 roop 弄的，很明显不好，卡通脸通常检测不到。
- [x] https://www.animaze.us/  效果一般，模型低级，没有特效。只有 win平台，怕自己做得没它好，不过做的是 套个头套
- [x] 部署并测试 DCT-Net 3D cartoon，看看前后闪烁问题
- [ ] 确定最终方案
- [ ] 更改 demo 的 3D 模型
- [ ] 寻找 3D 模型绑定骨骼尝试
- [ ] snapchat
- [ ] 修改代码，捕获源为视频，更改前端页面（等后面做，先暂时流程通的）
- [ ] VToonify
- [ ] 弄 3D 模型，数字人，
- [ ] AE 特效？






（6）杂记

[一张图片，语音驱动形成动画](https://www.youtube.com/watch?app=desktop&v=FUi2kmVaxBU)，这个效果挺好的，但不适合这个场景

做好了，也是可以沉淀下来东西的，前端动捕，绑骨，渲染，CV，数字人，3D 制作，表情制作。

看友商，，虚幻引擎做的：https://www.virdync.com/h-col-194.html

数字人制作流程：形象制作 - 骨骼及表情绑定 - 动作表情捕捉 - 动捕数据精修 - 毛发布料解算 - （实拍，场景制作 - 镜头动画 - 角色动画，动画合成 - 渲染与后期特效）

这 sysmocap 也是，代码量好大，，


`0905`


>灯光渲染、纹理，对最终效果十分关键

- [ ] DCT-Net；暂不继续，搜 cartoon3Dstyle 就能出来类似的效果

- [ ] 修改代码，使其处理视频


- [ ] blender 基础
- [ ] bledner blendshape
- [ ] 切换模型尝试

- [ ] sysmocap electron 弄明白




## 2023 秋季

完善工具链，做成一个稳定的服务（完成一个工具系统，收益还是很大的）
- 语音合成（可能后续工作：改进load方式, 训底层tts，训音色 vc层）(试试后期调音， adobe 有个音效增强)（多语种支持）
- 视频合成（涉及 视频动捕、欧美动漫风模型、模型加载、更改配件、抠图等一系列工具）
- 视频素材检索（图搜视频，部署到 BI 系统）

优化的同时：整理文件结构，弄成文档

弄好之后：桌球代码了解优化


pytorch_p310【部署 vits-svc 服务】


## 2023 冬季

希望做个 electron/web 套 python ，万能软件开发，替share

完善 AIGC-AUDIO, AIGC 原画生成，AIGC 视频换脸